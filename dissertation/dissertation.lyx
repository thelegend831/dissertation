#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 
\end_preamble
\use_default_options true
\begin_modules
customHeadersFooters
minimalistic
todonotes
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref false
\pdf_title "Holoportation"
\pdf_author "Andy Pack"
\pdf_subject "The use of Kinect cameras to stream 3D video from client to server"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plain
\biblio_options urldate=long
\biblatex_bibstyle ieee
\biblatex_citestyle ieee
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2.2cm
\rightmargin 2cm
\bottommargin 2.2cm
\secnumdepth 4
\tocdepth 4
\paragraph_separation skip
\defskip medskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\bullet 1 0 9 -1
\bullet 2 0 24 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\size giant
Multi-Source Holoportation 
\begin_inset Newline newline
\end_inset

with LiveScan3D
\end_layout

\begin_layout Author
Andy Pack
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../surreylogo.png
	lyxscale 15
	width 20col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 4pheight%
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{changemargin}{3cm}{3cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center

\size large
A dissertation submitted to the Department of Electronic Engineering in
 partial fulfillment of the Degree of Masters of Engineering in Electronic
 Engineering
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{changemargin}
\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
Supervised by Professor Ning Wang
\end_layout

\begin_layout Standard
\noindent
\align center
May 2020
\size large

\begin_inset Newline newline
\end_inset

Department of Electrical and Electronic Engineering
\begin_inset Newline newline
\end_inset

Faculty of Engineering and Physical Sciences
\begin_inset Newline newline
\end_inset

University of Surrey
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Abstract
abstract
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\begin_inset CommandInset toc
LatexCommand lstlistoflistings

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
\noindent
\align center
I'd like to extend my thanks to Professor Ning Wang for both the learning
 opportunities provided by this project and his continued support.
 
\end_layout

\begin_layout Standard
\noindent
\align center
I would also like to thank Ioannis Selinis and Sweta Anmulwar for their
 shared patience and help throughout this year.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Right Footer
Andy Pack / 6420013
\end_layout

\begin_layout Left Footer
May 2020
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The immersive technology spaces of virtual and augmented reality promise
 to change the way we experience many different forms of media.
 Spurred in recent years by the release of consumer VR headsets and the
 push for handheld AR by mobile phone manufacturers, no longer do these
 experiences present merely proofs of concept but complete commercial products.
 
\end_layout

\begin_layout Standard
While some present natural extensions to existing technology as seen in
 VR gaming, others are more tied to the new domain.
 The power of modern smartphones has allowed augmented reality to be integrated
 into applications both as the primary function and in more secondary features
 such as visualising products in shopping apps like 
\noun on
IKEA Place
\noun default
.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
reference?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
No matter the application, common to all is the importance of the presented
 media itself.
 Typically this is in the form of pre-recorded meshes of 3D objects, captured
 and stored within the application for both the previously mentioned AR
 and VR examples.
 Less seen in commercial products is the live-streaming of 3D renders due
 in part to both the required network bandwidth and the hardware required
 to capture such media.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
why?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
One such technology for the capture and transmission of 3D video is 
\noun on
LiveScan3D
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d"
literal "false"

\end_inset

, a client-server suite of applications utilising the 
\noun on
Microsoft Kinect
\noun default
 camera to capture a scene in real-time and deliver the result to a server
 for reconstruction and presentation.
 Renders or 
\emph on
holograms
\emph default
 can then be delivered to a 
\emph on
user experience
\emph default
 such as an AR or VR client, together a process referred to as 
\emph on
holoportation
\emph default
 (hologram teleportation).
\end_layout

\begin_layout Standard
This project aims to extend this suite to support multi-source holoportation,
 receiving multiple scenes concurrently analogous to the move from traditional
 phone calls to group conference calls.
 In doing so the implementation of holoportation could be seen to be generalised
, extending the possible applications of the suite.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
examples?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As the spaces of augmented and virtual reality become more commonplace and
 mature, the ability to capture and stream 3D renders over the internet
 using consumer-grade hardware has many possible applications and presents
 one of the most direct evolutions of traditional video streaming.
\end_layout

\begin_layout Standard
A conceptual view of a multi-source configuration can be seen in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:premise"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Both single and multi-view sources are shown, the latter allowing more
 complete renders of the subject to be acquired from different angles.
 Both shapes are presented at the 
\emph on
user experience
\emph default
, typically envisaged as an AR or VR client.
\end_layout

\begin_layout Standard
The code for this project resides in two 
\noun on
Github
\noun default
 repositories the URLs for which can be seen below, the reports and associated
 materials including gathered data is also under source control.
\end_layout

\begin_layout Standard
\noindent
\align center

\noun on
LiveScan
\noun default
 Suite: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout
github.com/Sarsoo/LiveScan3D
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Mobile AR Application: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout
github.com/Sarsoo/LiveScan3D-Unity
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Report Materials: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout
github.com/Sarsoo/dissertation
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/premise.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Demonstration of a multi-source holoportation system including single and
 multiple view camera configurations
\begin_inset CommandInset label
LatexCommand label
name "fig:premise"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Objectives
\end_layout

\begin_layout Standard
In order to achieve the goal of multi-source holoportation the following
 key objectives must be achieved,
\end_layout

\begin_layout Enumerate
Redefine the network communications in order to identify each frame of footage
 as a particular source
\end_layout

\begin_layout Enumerate
Migrate the server from a single-source streaming node to a manager of multiple
 scenes
\end_layout

\begin_layout Enumerate
Extend the native viewfinder of the server in order to separately render
 each connected source
\end_layout

\begin_layout Enumerate
Update the 
\noun on
Android
\noun default
 AR application to separately present each source of footage and facilitate
 individual control
\end_layout

\begin_layout Standard
These can be seen contextually highlighted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:High-level-objectives"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ObjectivesSummary.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
High level diagram of the suite with key objectives highlighted
\begin_inset CommandInset label
LatexCommand label
name "fig:High-level-objectives"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
COVID-19
\begin_inset CommandInset label
LatexCommand label
name "subsec:COVID-19"

\end_inset


\end_layout

\begin_layout Standard
Conducted throughout the 2019/20 academic year, the project was inevitably
 affected by the global COVID-19 pandemic.
 With a lack of access to the on-campus lab environment, from March onwards
 there was access to only a single 
\noun on
Kinect
\noun default
 sensor and no access to an 
\noun on
Android
\noun default
 phone with AR support.
 This significantly hindered the ability to evaluate the implemented multi-sourc
e capabilities and some elements of the mobile AR display, as a result alternati
ve objectives were identified to allow quantitative analysis.
\end_layout

\begin_layout Standard
In order to continue debugging the mobile AR user experience the structure
 of the 
\noun on
Unity
\noun default
 application was changed significantly in order to allow deployment on either
 an Android or iOS device.
 This allowed the network behaviour to continue being evaluated.
\end_layout

\begin_layout Standard
In support of the lab's ongoing research into the 
\noun on
LiveScan
\noun default
 suite and the area of theory in which it resides, investigations were made
 into the effect of deliberately limiting the delivered frames per second
 on effective display latency.
 Preliminary data for one method of doing so was gathered and is presented
 here in place of proper evaluation of the completed multi-source capabilities.
\end_layout

\begin_layout Section
Literature Review
\end_layout

\begin_layout Standard

\noun on
LiveScan3D
\noun default
 utilises the 
\noun on
Microsoft Kinect
\noun default
 sensor to capture RGB video with depth information.
 While Kinect sensors have proved extremely popular in the computer vision
 sector, it does not represent the only method for such 3D reconstruction,
 traditional visual hull reconstruction is investigated before identifying
 the 
\noun on
Kinect
\noun default
's role in this space.
\end_layout

\begin_layout Standard
The significance of 3D video like that captured and relayed using the 
\noun on
LiveScan
\noun default
 suite is related to the development of new technologies able to immersively
 display such video content.
 
\end_layout

\begin_layout Standard
While this has been exemplified mostly through AR with 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-hololens"
literal "false"

\end_inset

's 
\noun on
LiveScan
\noun default
 client for 
\noun on
Microsoft Hololens
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-android"
literal "false"

\end_inset

's extension of this for 
\noun on
Android
\noun default
 phones
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-android"
literal "false"

\end_inset

, the collection and transmission of 3D holograms have applicability to
 all forms of XR and as such the state of this space as a whole is investigated.
\end_layout

\begin_layout Standard
As the foundation of this project, the 
\noun on
LiveScan3D 
\noun default
suite itself is presented in more depth following this review in order to
 contextualise it both within these investigations and the extension work
 presented herein.
\end_layout

\begin_layout Subsection
3D Capture & Reconstruction
\end_layout

\begin_layout Subsubsection
Visual Hull Reconstruction
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
CVSSP case study from CV?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
RGB-D Cameras
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Structure Sensor
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Initially designed as a motion control accessory for the 
\noun on
Xbox
\noun default
, the 
\noun on
Kinect
\noun default
 is a series of depth aware cameras produced by 
\noun on
Microsoft
\noun default
.
 The device uses additional infrared lights and sensors alongside an RGB
 camera in a configuration referred to as a Time-of-Flight camera to generate
 3D renders of a surroundings.
 The device also includes motion tracking and skeleton isolation for figures
 in view.
\end_layout

\begin_layout Standard
Following the release of an SDK for Windows in 2012, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "original-kinect-microsoft"
literal "false"

\end_inset

 at 
\noun on
Microsoft Research
\noun default
 reflects on the original camera's capabilities and the applications to
 computer vision research in 
\begin_inset CommandInset citation
LatexCommand cite
key "original-kinect-microsoft"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Here 3D conference calling of the type described in the introduction without
 AR or VR applications is presented, instead users watch a composite conference
 space on a screen with all participants rendered within.
 Work was undertaken to achieve mutual gaze between participants, a marked
 advantage over traditional conference calls where the lack of such aspects
 of group interaction makes the experience more impersonal.
 Methods of achieving more natural virtual interactions or 
\emph on
telepresence
\emph default
 are covered in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Holoportation-and-Telepresence"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
A second version of the camera, v2, was released alongside the 
\noun on
Xbox One
\noun default
 in 2013 and presented many improvements over the original.
 A higher-quality RGB camera captures 1080p video at up to 30 frames per
 second with a wider field of view than the original
\begin_inset CommandInset citation
LatexCommand cite
key "kinect-specs"
literal "false"

\end_inset

.
 The physical capabilities of the camera are discussed by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "new-kinect"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "new-kinect"
literal "false"

\end_inset

.
 The second version of the camera was found to gather more accurate depth
 data than the original and was less sensitive to daylight.
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "kinectv1/v2-accuracy-precision"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kinectv1/v2-accuracy-precision"
literal "false"

\end_inset

 found similar results with the v2 achieving higher accuracy results over
 the original.
 The second version did, however, achieve lower precision results than the
 v1 with recommendations made to include pre-processing on acquired depth
 images to control for random noise, 
\emph on
flying pixels
\emph default
 and 
\emph on
multipath interference
\emph default
.
\end_layout

\begin_layout Standard
The 
\noun on
Kinect
\noun default
 is used successfully by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "greenhouse-kinect"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "greenhouse-kinect"
literal "false"

\end_inset

 for object detection in the context of an autonomous vehicle navigating
 a greenhouse.
 The depth information was used in conjunction with the RGB information
 to identify obstacles, while the paper lays out some limitations of the
 camera it was found to be effective in it's aim and was capable of running
 on a reasonable computer.
\end_layout

\begin_layout Standard
This second iteration on the 
\noun on
Kinect
\noun default
 is frequently used in computer vision experiments with many of the works
 cited here using it for acquisition.
\end_layout

\begin_layout Subsection
Extended Reality (XR)
\end_layout

\begin_layout Standard
Immersive media experiences enhanced through the use of technology are typically
 defined by the level to which they affect the perception of the user.
 This distinction typically organises technologies into one of three established
 terms, 
\emph on
Virtual Reality
\emph default
, 
\emph on
Augmented Reality
\emph default
 and 
\emph on
Mixed Reality
\emph default
.
\end_layout

\begin_layout Description
Virtual The replacement of a user's experience of unmediated reality, rendering
 a new computer-generated space that the user appears to immersively inhabit.
 Typically achieved through face mounted headsets (
\emph on
Facebook Oculus, HTC Vive, Playstation VR, Valve Index
\emph default
).
\end_layout

\begin_layout Description
Augmented The enhancement of a user's reality through the overlay of digital
 graphics.
 Typically facilitated with translucent/transparent headsets 
\emph on
(Microsoft Hololens, Google Glass)
\emph default
 or increasingly with 
\begin_inset Quotes eld
\end_inset

Window on the World
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "reality-virtuality-continuum"
literal "false"

\end_inset

 mobile technologies 
\emph on
(Android ARCore
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "ARCore"
literal "false"

\end_inset

, 
\emph on
Apple ARKit
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "arkit"
literal "false"

\end_inset


\emph on
)
\emph default
 such as 
\emph on
Pokemon GO
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "pokemonGO"
literal "false"

\end_inset

.
\end_layout

\begin_layout Description
Mixed A combination of virtual elements with the real world to facilitate
 interaction with an augmented reality.
 A somewhat broad term owing to its description of a point between augmented
 and virtual reality.
 An emphasis is typically placed on virtual elements existing coherently
 within the real world and interacting in real-time.
\end_layout

\begin_layout Standard
The term 
\emph on
Extended Reality 
\emph default
or XR functions as an umbrella term for all such experiences and is used
 throughout this paper, note that the terms 
\emph on
mediated reality
\emph default
 and *R
\begin_inset CommandInset citation
LatexCommand cite
key "all-reality"
literal "false"

\end_inset

 are also sometimes used where the asterisk refers to 
\begin_inset Quotes eld
\end_inset

all
\begin_inset Quotes erd
\end_inset

 realities.
 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Cross reality? Reference?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While individual classes of XR provide ostensibly different experiences,
 it can be seen that there is overlap between them, notably that at a high
 level all aim to extend a user's experience of reality.
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Reword
\end_layout

\end_inset

 All can be seen to employ 
\emph on
Spatial Computing
\emph default
 as defined by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "spatial-computing"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "spatial-computing"
literal "false"

\end_inset

 to refer to 
\end_layout

\begin_layout Quote

\emph on
Human interaction with a machine in which the machine retains and manipulates
 referents to real objects and spaces.
\end_layout

\begin_layout Standard
Identifying the common dimensions across XR has led to the proposal of various
 taxonomies providing insights into how each implementation relate to others
\begin_inset CommandInset citation
LatexCommand cite
key "reality-virtuality-continuum,mr-taxonomy,all-reality"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
The Reality Virtuality Continuum
\end_layout

\begin_layout Subsubsection
XR Implementations
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Mobile AR examples
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Although VR and AR headsets have accelerated the development of XR technology,
 they are not the only way to construct XR experiences.
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "roomalive"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "roomalive"
literal "false"

\end_inset

 demonstrate 
\emph on
RoomAlive
\emph default
, an AR experience using depth cameras and projectors (referred to as 
\emph on
procams
\emph default
) to construct experiences in any room.
 This is presented through games and visual alterations to the user's surroundin
gs.
 A strength of the system is it's self-contained nature, able to automatically
 calibrate the camera arrangements using correspondences found between each
 view.
 Experience level heuristics are also discussed regarding capturing and
 maintaining user attention in an environment where the experience can be
 occurring anywhere, including behind the user.
 
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Link with work
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A point is also made about how the nature of this room based experience
 breaks much of the typical game-user interaction established by virtual
 reality and video games.
 In contrast to traditional and virtual reality game experiences where the
 game is ultimately in control of the user or user avatar, AR experiences
 of this type have no physical control over the user and extra considerations
 must be made when designing such systems.
\end_layout

\begin_layout Standard
Traditional media consumption is not the only area of interest for developing
 interactive experiences, an investigation into the value of AR and VR for
 improving construction safety is presented by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "ar/vr-construction"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "ar/vr-construction"
literal "false"

\end_inset

.
 A broad look at the applicability is taken with assessments including VR
 experiences for developing worker balance to aid in working at elevation
 and AR experiences incorporated into the workplace for aiding in task sequencin
g to reduce the effect of memory on safety.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Link with work
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "remixed-reality"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "remixed-reality"
literal "false"

\end_inset

 demonstrate an example of mixed reality through the use of 
\noun on
Kinect
\noun default
 cameras and a virtual reality headset.
 Users are placed in a virtual space constructed from 3D renders of the
 physical environment around the user.
 Virtual manipulation of the space can then be achieved with visual, spatial
 and temporal changes supported.
 Objects can be scaled and sculpted in realtime while the environment can
 be paused and rewinded.
 The strength of mixed reality comes with the immersion of being virtually
 placed in a version of the physical surroundings, tactile feedback from
 the environment compounds this.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Link with work
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Augmented Reality
\end_layout

\begin_layout Standard
The advancement of mobile AR aided by it's accessibility without expensive
 ancillary hardware has led it to be a rapidly growing and popular form
 of XR.
 The introduction of OS-level SDK's in 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "ARCore"
literal "false"

\end_inset

 and 
\noun on
Apple
\noun default
's 
\noun on
ARKit
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "arkit"
literal "false"

\end_inset

 make it easier to create AR experiences without the need to re-implement
 the required boilerplate computer vision concepts.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Handheld and Hololens
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Mobile AR SDKs and Unity ARFoundation
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Holoportation and Telepresence
\begin_inset CommandInset label
LatexCommand label
name "subsec:Holoportation-and-Telepresence"

\end_inset


\end_layout

\begin_layout Standard
The term Holoportation is defined and exemplified in a 
\noun on
Microsoft Research
\noun default
 paper by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "holoportation"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "holoportation"
literal "false"

\end_inset

 where an end-to-end pipeline is laid out for the acquisition, transmission
 and display of 3D video facilitating real-time AR and VR experiences.
 The 
\noun on
Microsoft Research
\noun default
 paper builds on works including by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset

 2 years earlier which describes attempts at achieving 
\begin_inset Quotes eld
\end_inset


\emph on
telepresence
\emph default

\begin_inset Quotes erd
\end_inset

, a term coined by Marvin Minksy to describe the transparent and intuitive
 remote control of robot arms as if they were the controllers own
\begin_inset CommandInset citation
LatexCommand cite
key "marvin-minksy"
literal "false"

\end_inset

.
 The term was broadened by Bill Buxton
\begin_inset CommandInset citation
LatexCommand cite
key "buxton-telepresence"
literal "false"

\end_inset

 to include the space of telecommunications to describe technology being
 used to make someone feel present in a different environment.
 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Is telepresence relevant here? reverse telepresence for something else being
 telepresent in your space?
\end_layout

\end_inset

In the context of holoportation this is through the use of 3D video reconstructi
on.
 The aforementioned work by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset

 used 10 
\noun on
Kinect
\noun default
 cameras to capture a room before virtually reconstructing the models.
 
\end_layout

\begin_layout Standard
In service of demonstrating it's applicability to achieving 
\emph on
telepresence
\emph default
, a figure was isolated from the surroundings and stereoscopically rear-projecte
d onto a screen for a single participant, a result of this can be seen in
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:stereoscopic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/telepresence-stereoscopic.png
	lyxscale 30
	width 40col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of stereoscopic projection of depth aware footage captured by
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:stereoscopic"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\noun on
Microsoft Research
\noun default
 paper demonstrates a system using 8 cameras surrounding a space.
 Each camera captured both Near Infra-Red and colour images to construct
 a colour-depth video stream, a more complex camera configuration than in
 the others cited.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "velt"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "velt"
literal "false"

\end_inset

 demonstrates a similar holoportation experience to 
\noun on
LiveScan3D
\noun default
 capable of supporting multi-view configurations, it also supports both
 point clouds and meshes.
 Calibrating multiple view points is completed using the extrinsics and
 intrinsics of the camera.
 The extrinsics are the relative positions of each 
\noun on
Kinect
\noun default
 camera while the intrinsics describe the internal properties of each camera,
 the focal length and optical centre.
 
\end_layout

\begin_layout Standard
The intrinsics of the 
\noun on
Kinect
\noun default
 camera can be retrieved from the 
\noun on
Kinect
\noun default
 SDK while the extrinsics are obtained in one of two ways.
 Extrinsics can be imported and parsed from XML for manual selection or
 estimated using 
\noun on
OpenCV
\noun default
 and a checkerboard pattern.
 When considering holoportation systems of this kind, comparatively few
 implement multiple views as a result the increased complexity involved
 in calibration.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Link to livescan?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multi-Source Holoportation
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
More?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The space of multi-source holoportation has been explored by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

 in the context of shared architectural design spaces in virtual reality
 similar to a conference call.
 Two groups of people were captured in 3D using clusters of 
\noun on
Kinect
\noun default
 cameras before having these renders transmitted to the other group.
 Each group reconstructs the other's render for display in virtual reality
 in conjunction with their own.
 In doing so a shared virtual space for the two groups has been created
 and it can be seen to implement the process of holoportation.
 The strength of the system as a shared architectural design experience
 is emergent of the semantics of the virtual space where a World in Miniature
 (WIM) metaphor is used.
\end_layout

\begin_layout Standard
The Worlds in Miniature is described by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "wim"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "wim"
literal "false"

\end_inset

 as a set of interfaces between the user and the virtual space they experience
 using tactile and visual tools.
 The interface involves providing the user with a miniature render of the
 world they are inhabiting that can interacted with in order to affect the
 full scale environment around them.
\end_layout

\begin_layout Standard
This navigation tool maps well to 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset

's
\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

 architecture groupware design, an image captured during the work can be
 seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:World-in-Miniature-group-by-group"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/group-by-group.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
World in Miniature render demonstrated in a multi-source holoportation context
 by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:World-in-Miniature-group-by-group"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
High Bandwidth Media Streaming
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
RTP
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
UDP
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
4K media streaming?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Compression? ZSTD
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Section
LiveScan3D
\begin_inset CommandInset label
LatexCommand label
name "sec:LiveScan3D"

\end_inset


\end_layout

\begin_layout Standard

\noun on
LiveScan3D
\noun default
 is a suite of software developed by Marek Kowalski, Jacek Naruniec and
 Michal Daniluk of the Warsaw University of Technology in 2015
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d"
literal "false"

\end_inset

.
 The suite utilises the 
\noun on
Xbox Kinect
\noun default
 v2 camera to record and transmit 3D renders over an IP network.
 A server can manage multiple clients simultaneously in order to facilitate
 multi-view configurations, it is then responsible for displaying the renderings
 in real-time and/or transmitting composite renders to a user experience
 or UE.
 This architecture can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LiveScanArchitecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
These renderings take the form of a point cloud, a collection of 3D co-ordinates
 each with an associated RGB colour value.
 There are many methods by which point clouds can be used to construct surfaces
 suited for traditional computer graphics applications
\begin_inset CommandInset citation
LatexCommand cite
key "point-cloud-surface"
literal "false"

\end_inset

 however for the purposes of an interactive or real-time application the
 plotting of each point of the cloud in a 3D space using a suitable point
 size can create a coloured mesh visually representing the captured object
 while keeping the processing pipeline streamlined.
 This is the approach taken in 
\noun on
LiveScan
\noun default
.
\end_layout

\begin_layout Standard
As a result of it's analogous nature to a traditional frame of 2D video,
 the terms 
\begin_inset Quotes eld
\end_inset

render
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

point cloud
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

frame
\begin_inset Quotes erd
\end_inset

 are used interchangeably from here.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/LiveScanArchitecture.png
	lyxscale 50
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture of the 
\noun on
LiveScan3D
\noun default
 suite
\begin_inset CommandInset label
LatexCommand label
name "fig:LiveScanArchitecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 Client
\end_layout

\begin_layout Standard
The 
\noun on
LiveScan
\noun default
 Client is responsible for interfacing with the 
\noun on
Kinect
\noun default
 sensor via the 
\noun on
Kinect
\noun default
 v2 SDK and transmitting hologram frames to the 
\noun on
LiveScan
\noun default
 Server.
 Body detection takes place client side, as does calibration when using
 multiple sensors.
\end_layout

\begin_layout Standard
Only one 
\noun on
Kinect
\noun default
 sensor can be connected to each computer as a result of the SDK's restrictions.
 A system used by multiple clients in this way lends itself well to multi-source
 configurations over the internet.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Extend
\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 Server
\end_layout

\begin_layout Standard
The server component of the 
\noun on
LiveScan
\noun default
 suite is responsible for managing and receiving 3D renders from connected
 clients.
 These holograms are reconstructed in an interactive 
\noun on
OpenGL 
\noun default
window functioning in a similar fashion to that of a traditional camera
 viewfinder allowing live previews of received data.
 Holograms can then be transmitted to the user experience or UE, constituting
 an XR client such as the 
\noun on
Hololens
\noun default
 or 
\noun on
Android
\noun default
 app.
 
\end_layout

\begin_layout Standard
A hologram frame as defined at the server is made up of the following,
\end_layout

\begin_layout Itemize
A list of 3D vertices
\end_layout

\begin_layout Itemize
A list of associated RGB values for each vertex
\end_layout

\begin_layout Itemize
A list of camera poses defining each 
\noun on
Kinect
\noun default
 sensors position and orientation
\end_layout

\begin_layout Itemize
A list of bodies, skeletons as identified by the 
\noun on
Kinect
\noun default
 sensor
\end_layout

\begin_layout Standard
The existing codebase shared variables for each between the main window
 and the 
\noun on
OpenGL
\noun default
 window, clearing and updating each with received live frames.
\end_layout

\begin_layout Standard
This structure can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:server-structure"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/initial-state.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial architecture of the 
\noun on
LiveScan3D
\noun default
 server
\begin_inset CommandInset label
LatexCommand label
name "fig:server-structure"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Calibration & Multi-View Configurations
\end_layout

\begin_layout Standard
Single view configurations provide one viewing angle of a captured scene,
 this results in areas of un-captured space causing visible shadows in the
 hologram.
 Capturing multiple angles of the same scene allows a more complete composite
 hologram to be constructed and presented.
\end_layout

\begin_layout Standard
When using a single client setup, frames are transmitted in their own coordinate
 space with the origin defined as the 
\noun on
Kinect
\noun default
 camera and the captured scene rendered in front of it.
\end_layout

\begin_layout Standard
When using multiple sensors, the server would be unable to combine these
 unique Euclidean spaces without knowledge of the sensors positions relative
 to each other, the 
\emph on
extrinsics
\emph default
 of the system.
\end_layout

\begin_layout Standard
In order to make a composite frame a calibration process is completed client-sid
e following instruction by the server.
\end_layout

\begin_layout Standard
Calibration is completed in two steps, an initial estimation followed by
 a refinement process.
 The initial estimation is completed by informing the server of which calibratio
n marker layouts are being used within the space.
 Client's identify possible visible markers like that seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:calibration-marker"
plural "false"
caps "false"
noprefix "false"

\end_inset

 using thresholding.
 Following this identification, the location of the marker can be found
 within the sensors coordinate system.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andy/uni/dissertation/media/calibration.png
	lyxscale 30
	width 20col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example marker used within the LiveScan3D calibration process
\begin_inset CommandInset label
LatexCommand label
name "fig:calibration-marker"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This information can be used to transform points from the cameras coordinate
 system to the marker's frame of reference.
 As the relative locations of different markers are defined at the server,
 a world coordinate system can be defined as the centre of these markers.
 Typically 4 different markers are placed on the faces around the vertical
 axis of a cuboid allowing views in 360°.
\end_layout

\begin_layout Standard
This world coordinate space has shifted the origin from being the position
 of the single 
\noun on
Kinect
\noun default
 sensor to being a point in the centre of the calibration markers that each
 camera now orbits.
 As part of this calibration process the server distributes transformations
 to each client defining where they sit within this world coordinate space.
 Client's can now transform acquired renders from their frame of reference
 to the world coordinate system at the point of capture and each point cloud
 can be merged coherently.
\end_layout

\begin_layout Standard
The refinement process is completed server-side by requesting a single frame
 from each connected client and using Iterative Closest Points
\begin_inset CommandInset citation
LatexCommand cite
key "ICP"
literal "false"

\end_inset

 (ICP) to improve the inter-camera relationships.
\end_layout

\begin_layout Standard
The 
\noun on
OpenGL
\noun default
 display space has it's origin within the centre of the visible box, this
 means that for single sensor setups this is also the location of the camera.
\end_layout

\begin_layout Subsection
Buffers and a non-blocking Network
\begin_inset CommandInset label
LatexCommand label
name "subsec:Buffers"

\end_inset


\end_layout

\begin_layout Standard
One of the weaknesses of the native codebase laid in it's network behaviour.
 Requests for live frames made by the server to each connected client were
 done in a synchronous fashion, pausing the operation of the thread responsible
 for passing these frames to the OpenGL display and UEs in order to wait
 for delivery from all clients.
\end_layout

\begin_layout Standard
While this would not be expected to present impaired performance in a controlled
 lab environment, it could prove detrimental when connected over the public
 internet where adverse network conditions and bandwidth bottlenecks are
 more likely.
\end_layout

\begin_layout Standard
In practice this means that the user experience both at the server and any
 connected UEs were determined by the instantaneous network conditions.
\end_layout

\begin_layout Standard
With these public internet applications and the expected network conditions
 in mind, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-buffers"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-buffers"
literal "false"

\end_inset

 provided improvements on this behaviour by including a system of buffers
 throughout the transmission pipeline in order to unblock the network operations.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
diagram?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Each client queues frames ready for transmission in a buffer while the server
 maintains a buffer at each socket for reception.
 The 
\noun on
KinectServer
\noun default
 collates frames from each socket's Rx buffer and moves them into a live
 buffer for display and moving into a Tx buffer ready for transmission onto
 any connected user experiences.
\end_layout

\begin_layout Standard
This system decouples the network behaviour smoothing the effect of temporary
 network conditions.
\end_layout

\begin_layout Standard
Additionally, Selinis' work included the extension of connections between
 both client-server and server-UE to allow the use of multiple concurrent
 TCP connections, effectively increasing the available bandwidth.
\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 XR Applications
\begin_inset CommandInset label
LatexCommand label
name "subsec:LiveScan-XR-Applications"

\end_inset


\end_layout

\begin_layout Standard
Developed at the start of 2017, the 
\noun on
LiveScan Hololens
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

 project provides an example of XR applications for the 
\noun on
LiveScan
\noun default
 suite.
 
\end_layout

\begin_layout Standard
Using the 
\noun on
Unity
\noun default
 game engine, the application allows the reception and reconstruction of
 
\noun on
LiveScan 
\noun default
point clouds in a head-mounted AR context.
\end_layout

\begin_layout Standard
Utilising a key strength of 
\noun on
Unity
\noun default
's native cross-platform capabilities 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-android"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-android"
literal "false"

\end_inset

 extended the 
\noun on
Hololens
\noun default
 application to build a handheld AR experience targeting the 
\noun on
Android
\noun default
 mobile operating system.
\end_layout

\begin_layout Standard
This was achieved through the static linking of 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 library
\begin_inset CommandInset citation
LatexCommand cite
key "arcore-unity"
literal "false"

\end_inset

 for 
\noun on
Unity
\noun default
 and included buffers inline with the developments discussed in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Buffers"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\noun on
LeanTouch
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "lean-touch"
literal "false"

\end_inset

 
\noun on
Unity
\noun default
 package was used to allow the live manipulation of displayed holograms,
 abstracting away much of the otherwise required boilerplate input processing
 and management.
\end_layout

\begin_layout Standard
Hologram point clouds are rendered in a similar fashion to the server's
 OpenGL window in that each vertex of the cloud is rendered individually
 creating the impression of a contiguous mesh as opposed to forming a coherent
 surface.
 The 
\noun on
PointCloudElem
\noun default
 game object with a customisable colour acts as the primitive vertex of
 this cloud.
\end_layout

\begin_layout Standard
The most relevant scripts for the developments are as follows,
\end_layout

\begin_layout Description
PointCloudReceiver Server-like object responsible for managing connections
 and sockets.
 Responsible for passing received point clouds from the 
\noun on
RxBuffer
\noun default
 to the 
\noun on
PointCloudRenderer
\noun default
 for presentation.
\end_layout

\begin_layout Description
PointCloudRenderer Display manager responsible for managing the live 
\noun on
PointCloudElem
\noun default
's representing a hologram.
 Takes de-buffered frames from the 
\noun on
PointCloudReceiver
\noun default
 and increases or decreases the population of 
\noun on
PointCloudElem
\noun default
's as required before using the contained 
\noun on
ElemRenderer
\noun default
 to update the colour and position of each.
\end_layout

\begin_layout Description
ElemRenderer A component of the 
\noun on
PointCloudElem
\noun default
 responsible for updating the presentation of each vertices' mesh.
\end_layout

\begin_layout Description
PointCloudSocket Traditional socket object for managing network connections.
 Buffers lists of vertices and RGB data from raw bytes received from the
 network.
\end_layout

\begin_layout Subsection
Evaluation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
Here an evaluation of the 
\noun on
LiveScan
\noun default
 suite is presented.
 Its strengths within the space of 3D capture and transmission are identified
 while it's limitations are also highlighted.
\end_layout

\begin_layout Standard
The main strength of the 
\noun on
LiveScan
\noun default
 suite lies in its display agnostic architecture.
 While some of the methods previously reviewed present domain-specific implement
ations of holoportation, such as 
\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

, 
\noun on
LiveScan
\noun default
 presents a generic way to capture and transmit 3D renders over an IP network
 in real-time.
 The transfer server then provides an interface for different display methods,
 theoretically most forms of XR, to connect and receive these holograms.
\end_layout

\begin_layout Standard
In concept, this design pattern makes it a good candidate for use in a hologram
 streaming service whether for personal communications similar to
\noun on
 Skype
\noun default
 or public broadcast akin to 
\noun on
Twitch
\noun default
 or 
\noun on
Instagram
\noun default
's live functionality.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
reference?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A limitation of the suite could be identified in its network protocol use,
 TCP connections are used throughout the communication pipeline from 
\noun on
Kinect
\noun default
 sensor to UE.
 While this connection-oriented protocol provides advantages through flow
 control, packet ordering and error handling, these also include overhead
 which limits the speed of transmission.
 For these reasons UDP is typically better suited for media streaming, especiall
y when in real-time.
 Investigations could be made into the suitability for its use in the 
\noun on
LiveScan3D
\noun default
 suite.
\end_layout

\begin_layout Standard
The addition of buffers allows the presentation layer to request frames
 from a buffer at a constant rate, decoupled from a level of volatility
 in the network layer.
 In doing so, however, the total perceived delay between capture and display
 can be increased as the frames spend time in each of the consecutive buffers.
 In an effort to identify methods for controlling this, investigations are
 made into the effect of intentionally reducing the transmitted frame rate
 in order to reduce this overall delay.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
comparison between multi-view and multi-source
\end_layout

\end_inset


\end_layout

\begin_layout Section
Developments
\end_layout

\begin_layout Standard
The developments made throughout this project to facilitate multi-source
 experiences were focused on four aspects of the suite,
\end_layout

\begin_layout Itemize
The native display element of the server was updated in order to allow the
 concurrent display of multiple sources.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This included a system of geometric transformations to facilitate the coherent
 arrangement of holograms in the 3D rendered space and a control scheme
 with which to manipulate them.
\end_layout

\end_deeper
\begin_layout Itemize
The network communications throughout the suite were extended to include
 source IDs.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Using this ID, both the server and display elements are able to differentiate
 frames for different sources during processing and presentation.
\end_layout

\end_deeper
\begin_layout Itemize
The mobile AR application was extended in order to display multiple holograms
 simultaneously.
\end_layout

\begin_layout Itemize
Additional features facilitating the paradigm shift away from a single stream
 to a multi-stream scenario.
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Stale 
\emph default
sources for which no network traffic is received within a timeout are identified
 and removed at the server.
\end_layout

\begin_layout Itemize
The global 
\noun on
KinectSetting
\noun default
s object describing parameters including the positions of calibration markers
 was made a source-level object in order to allow multi-view configurations.
\end_layout

\end_deeper
\begin_layout Standard
Additionally, two other developments were made to the application suite
 (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:COVID-19"
plural "false"
caps "false"
noprefix "false"

\end_inset

, COVID-19),
\end_layout

\begin_layout Itemize
A dynamic system of frame rate throttling was implemented.
\end_layout

\begin_deeper
\begin_layout Itemize
This was in order to facilitate investigations into the effect on user experienc
e of balancing FPS and display latency.
\end_layout

\end_deeper
\begin_layout Itemize
The mobile UE application was refactored in order to allow cross-platform
 deployment.
\end_layout

\begin_layout Subsection
Server Display
\end_layout

\begin_layout Standard
This section presents the extensions made to the presentation layer of the
 server application.
 
\end_layout

\begin_layout Standard
The semantics for the implemented geometric transformations are explored
 through the use of the 
\noun on
Transformer
\noun default
 and 
\noun on
DisplayFrameTransformer
\noun default
 objects.
 The final implementation of this geometry within the code is presented.
 The new control scheme is presented, it's weaknesses are considered.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
In designing the extension to the server's OpenGL display window, geometric
 transformations were used in order to arrange holograms in the virtual
 space through rotation and translation.
 This defines each hologram's position within the viewfinder's coordinate
 space, by default arranging them in a circle around the origin.
\end_layout

\begin_layout Standard
In order to maintain a strength of the suite in it's agnostic behaviour
 to different display methods, it was made critical to the design of the
 OpenGL extension that spatial processing would be specific to this display
 only.
 Mobile AR applications will arrange and interact with holograms in a different
 way, as will head-mounted AR or VR applications.
 In maintaining this separation of network and presentation layer each display
 can operate independently and no assumptions are made as to how a display
 method should handle each hologram.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
During initial testing frames received from a live sensor were intercepted
 and serialized to XML in local storage.
 These frames were then loaded into memory as the server was started and
 merged with those received live before display.
\end_layout

\begin_layout Standard
The composite frame can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-composite-frame"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/pretransform.jpg
	lyxscale 10
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial multi-source composite testing frame
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-composite-frame"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The objects can be seen to be occupying the same space due to their similar
 positions in the frame during capture.
 This is not a sufficient solution for displaying separate sources and so
 geometric transformations like those described above were employed to separate
 the two.
 The change in software structure at this stage can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-testing-layout"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 A rotation of 180° in the vertical (
\begin_inset Formula $y$
\end_inset

) axis pivoted the frames such that they faced those being received live,
 the results can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:180-degree-rotation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/local-testing.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial testing process transforming frames loaded from local storage
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-testing-layout"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/180flip.jpg
	lyxscale 10
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Composite testing frame following 180° rotation of recorded source in 
\begin_inset Formula $y$
\end_inset

 axis
\begin_inset CommandInset label
LatexCommand label
name "fig:180-degree-rotation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to follow this design the transformations were moved to instead
 occur within the 
\noun on
OpenGL
\noun default
 window class.
 To allow this the shared variables between the 
\noun on
MainWindow
\noun default
 and 
\noun on
OpenGL
\noun default
 were changed.
 A Frame object was defined to wrap an individual point cloud with a source
 ID to allow differentiation, the definition can be seen in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Frame"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The object holds fields for each of the lists previously shared between
 the two objects including a list of vertices (co-ordinates) and the RGB
 values for each as well as the camera poses and bodies.
\end_layout

\begin_layout Standard
The original 
\noun on
LiveScan3D
\noun default
 cleared each of these variables before retrieving a new frame, when moving
 to a multi-source architecture the ability to individually update source
 point clouds was prioritised.
 This would avoid blocking the entire display when unable to receive frames
 from a specific client, other clients would still be able to have frames
 updated promptly.
\end_layout

\begin_layout Standard
To accomplish this a dictionary was used as the shared variable with each
 client's frame referenced by its source ID.
 In doing so only one frame per client is kept and each new frame overrides
 the last.
 During rendering the dictionary is iterated through and each point cloud
 combined.
 During combination a client-specific transformation is retrieved from an
 instance of the 
\noun on
DisplayFrameTransformer
\noun default
 class.
 This object is a member of the 
\noun on
OpenGL
\noun default
 window and is responsible for defining the orientation and position of
 each point cloud.
\end_layout

\begin_layout Subsubsection
Geometric Transformations
\end_layout

\begin_layout Standard
Within the 
\noun on
LiveScan3D
\noun default
 server source code are utility structures and classes which were extended
 in order to develop a wider geometric manipulation system.
 Structures defining Cartesian coordinates in both 2D and 3D spaces called
 
\noun on
Point2f
\noun default
 and 
\noun on
Point3f
\noun default
 respectively are used in drawing skeletons as captured by the 
\noun on
Kinect
\noun default
 camera, there is also a class defining an affine transformation.
\end_layout

\begin_layout Standard
Affine transformations are a family of geometric transformations that preserve
 parallel lines within geometric spaces.
 Some examples of affine transformations include scaling, reflection, rotation,
 translation and shearing.
\end_layout

\begin_layout Standard
The class definition is made up of a three-by-three transformation matrix
 and a single 3D vector for translation, within the original codebase it
 is used for both camera poses and world transformations.
 
\end_layout

\begin_layout Standard
A camera pose is the affine transformation defining the position and orientation
 of the 
\noun on
Kinect
\noun default
 camera when drawn in the 
\noun on
OpenGL
\noun default
 space as a green cross.
 The world transformations are used as part of the calibration process when
 using multi-view configurations.
\end_layout

\begin_layout Standard
When considering how each source's render would be arranged in the space,
 the use of this class definition was extended.
 As the use of affine transformations is mostly limited to use as a data
 structure within the base source code, some utility classes and functions
 were required in order to fully maximise their effectiveness.
\end_layout

\begin_layout Paragraph

\noun on
Transformer
\end_layout

\begin_layout Standard
The motivation in writing the 
\noun on
Transformer
\noun default
 was to create a generic framework of geometric transformations that could
 be utilised by the 
\noun on
OpenGL
\noun default
 display to arrange separate point clouds.
 At a high level this is done by implementing matrix arithmetic functions
 in the context of their use for applying linear transformations to Cartesian
 coordinates.
 
\end_layout

\begin_layout Standard
The 
\noun on
Transformer
\noun default
 class has static methods to apply 
\noun on
AffineTransform
\noun default
s to both 
\noun on
Point3f
\noun default
 structures and lists of raw vertices as received from 
\noun on
LiveScan
\noun default
 clients.
\end_layout

\begin_layout Standard
Additionally, utility functions to bidirectionally cast between 
\noun on
Point3f
\noun default
 data structures and the lists of received vertices were written.
\end_layout

\begin_layout Standard
Finally static methods generate common rotation transformations about each
 axis given an arbitrary angle.
 This provided a foundation on which to define how the 
\noun on
OpenGL
\noun default
 space would arrange separate sources within it's combined co-ordinate space.
\end_layout

\begin_layout Subsubsection

\noun on
DisplayFrameTransformer
\end_layout

\begin_layout Standard
The 
\noun on
DisplayFrameTransformer
\noun default
 is responsible for maintaining the geometric state of each source, generating
 transformations for each displayed within the 
\noun on
OpenGL
\noun default
 window at each frame update.
 A UML diagram for the class can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:UML-displayframetransformer"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/DisplayFrameTransformer.png
	lyxscale 20
	width 75col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
UML diagram for 
\noun on
DisplayFrameTransformer
\noun default

\begin_inset CommandInset label
LatexCommand label
name "fig:UML-displayframetransformer"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Each source is assigned a default transformation which can be overridden
 using keyboard controls.
\end_layout

\begin_layout Standard
Sources are initially arranged in a circle around the origin in the center
 of the space.
 This is done by retrieving a transformation from the 
\noun on
Transformer
\noun default
 for a rotation in the 
\begin_inset Formula $y$
\end_inset

-axis for each source, 
\begin_inset Formula $n$
\end_inset

.
 Each angle of rotation, 
\begin_inset Formula $\alpha$
\end_inset

, is calculated using the below, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha\left(n\right)=\frac{n}{\sum sources}\cdotp360\textdegree
\]

\end_inset


\end_layout

\begin_layout Standard
Similar to the shared variables between the 
\noun on
MainWindow
\noun default
 and 
\noun on
OpenGL
\noun default
 window, source transformations are stored within a dictionary indexed by
 source ID.
\end_layout

\begin_layout Standard
The 
\noun on
DisplayFrameTransformer
\noun default
 also has methods to override these initial transforms with the RotateSource()
 and TranslateSource() methods.
 When these methods are called for the first time on a source, an object
 defining the position and rotation is populated using the default rotation.
 From here the presence of a source override results in applied transforms
 being defined by these values as opposed to the default orientation.
\end_layout

\begin_layout Standard
This leaves the current architecture of the server application as described
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:current-state-diagram"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/december-state.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Current state of 
\noun on
LiveScan
\noun default
 server structure with 
\noun on
OpenGL
\noun default
 window-based transformer
\begin_inset CommandInset label
LatexCommand label
name "fig:current-state-diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Control Scheme
\end_layout

\begin_layout Standard
The movement of objects within the 
\noun on
OpenGL
\noun default
 space is implemented through keyboard controls.
 While using the mouse would allow fine-grained and intuitive control, the
 number of axes for motion and rotation available to objects makes defining
 specific keys for each more flexible.
 This additionally removes the need to redefine or overload the camera controls.
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

I
\begin_inset Quotes erd
\end_inset

 key is used to cycle through displayed sources, the currently selected
 source is the subject of each of the movement actions.
 Sources are moved across the horizontal plane (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $x$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $z$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
) of the display space using a WASD-esque layout of the UHJK keys.
 Objects can be rotated about the vertical (
\begin_inset Formula $y$
\end_inset

) axis using the B and N keys.
 Finally the placement of an object can be reset to default using the R
 key, the addition of the shift modifier resets all clients.
\end_layout

\begin_layout Standard
Worth noting is that this represents arbitrary placement of sources in only
 two axes of position and one of rotation.
 This was a conscious choice as these are the most common and intuitive
 axes with which sources will need to be manipulated.
 The ability to allow movement in all axes would require only binding these
 actions to keys.
\end_layout

\begin_layout Standard
There is room to improve these controls as the directions of movement are
 in relation to the fixed axes of the display space as opposed to the view
 of the viewpoint camera.
 In practice this means that when moving objects in the display space the
 orientation of the space must be considered in order to identify which
 direction the object should be moved.
 
\end_layout

\begin_layout Standard
This is less intuitive than could be expected in other areas where such
 a control scheme is used such as video games or modelling software.
 In such implementations when moving objects the directions are typically
 taken from the camera's frame of reference.
 The feasibility of employing a similar control philosophy should be considered.
\end_layout

\begin_layout Subsection
Network Source IDs
\end_layout

\begin_layout Standard
The following section explores how the network communications throughout
 the 
\noun on
LiveScan
\noun default
 suite were extended in order to identify the source each frame corresponds
 to.
\end_layout

\begin_layout Standard
The original and updated packet structures are explored.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
Within the 
\noun on
LiveScan
\noun default
 suite two network communications required updating, those between the client
 and server and those between the server and user experiences.
\end_layout

\begin_layout Standard
In the native codebase the headers for all packets were the same, the structure
 can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Existing-packet-header"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderSTD.png
	lyxscale 50
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Native packet header structure for all 
\noun on
LiveScan
\noun default
 TCP packets
\begin_inset CommandInset label
LatexCommand label
name "fig:Existing-packet-header"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The message type field indicates what action is being conducted with examples
 including a request for a frame from the server to the client or the delivery
 of settings.
 The message types can be seen in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Network-Message-Types"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 While there are many forms of message sent between client and server including
 settings delivery and calibration requests, messages from server to UE
 currently only correspond to frame delivery.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
An integer source ID was created to be included in communications throughout
 the suite.
 For conciseness this was represented by a single byte during transmission
 resulting in a range of possible source IDs between 0 and 127.
 Considering the use case of a source representing a scene and the bandwidth
 required for each, 128 possible concurrent sources was deemed sufficient.
\end_layout

\begin_layout Standard
When and where the source ID was included within each packet was decided
 by the semantics behind the different kinds of message.
 For server to UE transmissions the source ID was included within the header
 of the packet as can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:new-ue-packet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderUE.png
	lyxscale 50
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Updated packet structure for server to UE communications, blue highlight
 indicates new field
\begin_inset CommandInset label
LatexCommand label
name "fig:new-ue-packet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
With regards to client to server communications it was noted that not all
 message types have a natural association with the source itself.
 For example when the server requests a client to begin calibration there
 is no requirement for the source ID to be present.
 As such the source ID is instead included only during frame delivery from
 client to server at the start of the payload.
 This layout can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:new-client-server-packet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderClientServer.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Updated packet structure for client to server frame delivery, blue highlight
 indicates new field
\begin_inset CommandInset label
LatexCommand label
name "fig:new-client-server-packet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Mobile AR
\end_layout

\begin_layout Standard
The architecture of the mobile AR application can be divided into two areas
 of concern.
 The first is related to establishing the AR environment within the 
\noun on
Unity
\noun default
 scene including plane discovery, motion tracking and presenting the camera
 feed as the background.
 This is provided by the 
\noun on
Google ARCore
\noun default
 library and requires only configuring provided objects, no additional code
 is required.
 The other aspect is the 
\noun on
LiveScan
\noun default
 specific objects responsible for receiving and constructing holograms within
 the AR scene.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/UnityScriptsBefore.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Existing architecture of mobile AR components
\begin_inset CommandInset label
LatexCommand label
name "fig:Existing-scripts"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This 
\noun on
LiveScan
\noun default
 specific code can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Existing-scripts"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:LiveScan-XR-Applications"
plural "false"
caps "false"
noprefix "false"

\end_inset

 the 
\noun on
PointCloudReceiver
\noun default
 acts as the network layer passing frames from the receiving buffer to the
 
\noun on
PointCloudRenderer
\noun default
 for reconstruction in the form of a collection of 
\noun on
PointCloudElems
\noun default
.
 This collection of 
\noun on
PointCloudElem
\noun default
s is scaled in population to match the size of the hologram and then each
 is re-coloured and positioned on each frame update.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
The nature of developing in 
\noun on
Unity
\noun default
 allowed the restructuring of this architecture into one capable of supporting
 multi-source experiences with minimal code additions.
 A source prefab
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
does prefab need defining?
\end_layout

\end_inset

 was created with the intention of encapsulating the necessary components
 required to represent a whole source including it's presentation touch
 input management.
 The 
\noun on
PointCloudRenderer
\noun default
 was redefined as this presentation manager for a single source allowing
 it to own the 
\noun on
PointCloudElem
\noun default
s making up a hologram.
\end_layout

\begin_layout Standard
On each frame update the 
\noun on
PointCloudReceiver
\noun default
 retrieves the required source from a dictionary indexed by source ID or
 instantiates one if it is not available
\noun on
.

\noun default
 The render function of the source's contained renderer can then be called.
 This updated architecture can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Scripts-after"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/UnityScriptsAfter.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Restructured architecture of mobile AR components
\begin_inset CommandInset label
LatexCommand label
name "fig:Scripts-after"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Stale Source Culling
\end_layout

\begin_layout Standard
Transitioning the 
\noun on
LiveScan
\noun default
 suite to a multi-source architecture implies a significant difference in
 certain behaviours of the server.
 When encountering adverse network conditions in a single source scenario,
 the desired action could be to wait until transmissions from the client(s)
 can resume.
 With only a single stream, the alternative would be to quit the experience.
\end_layout

\begin_layout Standard
However in a multi-source scenario, a decision could be made to instead
 remove sources that cannot maintain a reasonable transmission rate so as
 not to infringe on the experience of other sources, no longer do the conditions
 of any single stream define the conditions of the whole.
\end_layout

\begin_layout Standard
A similarity could be drawn between watching a traditional broadcast sports
 game compared to 
\emph on
simulcast
\emph default
 broadcasts such as those seen on the 
\noun on
NFL's RedZone
\noun default
 where multiple games can be watched simultaneously dividing the screen.
 Were one of the games to experience transmission issues, it could be considered
 beneficial to the experience to remove the game from display and wait for
 the conditions to improve, especially in a commercial context.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
A source could be considered 
\emph on
stale
\emph default
 when a frame has not been received for it within a certain time threshold,
 the default was decided as 5 seconds.
 Beyond this the source should be considered, if at least temporarily, disconnec
ted.
\end_layout

\begin_layout Standard
This could be achieved using a separate thread that periodically iterates
 through the last frame of each source and compares an associated timestamp
 to the current time.
\end_layout

\begin_layout Standard
While this thread could be included in many places throughout the server
 application, it was considered important to facilitate applicability to
 any cross-platform developments.
\end_layout

\begin_layout Standard
As such the object-oriented programming concept of encapsulation was employed
 to create a object responsible for containing live frames and checking
 for 
\emph on
stale
\emph default
 sources.
\end_layout

\begin_layout Subsubsection

\noun on
SourceCollection
\end_layout

\begin_layout Standard
The 
\noun on
SourceCollection
\noun default
 object wraps around the dictionary object used to store live frames indexed
 by source ID.
 The previously described thread is implemented along with integer attributes
 defining the interval at which frames are checked and the threshold for
 which sources are designated 
\emph on
stale
\emph default
.
 The UML diagram for this object can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:source-collection"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/SourceCollection.png
	lyxscale 20
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
UML diagram for the 
\noun on
SourceCollection
\noun default

\begin_inset CommandInset label
LatexCommand label
name "fig:source-collection"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to lend the design to a cross-platform server, events are used
 to signal a source being connected or disconnected.
 A connected source is identified when a frame is added for which there
 is no key found in the sources dictionary.
 The disconnected source event fires during the RemoveSource() method which
 is called manually or by the cleaner thread.
\end_layout

\begin_layout Standard
An example of the use of these events is found in the 
\noun on
DisplayFrameTransfomer
\noun default
 where a check is made to ensure that a source's position override is cleared
 if one exists when that source is disconnected.
\end_layout

\begin_layout Subsection
Multi-Source Settings
\end_layout

\begin_layout Standard
The 
\noun on
KinectSettings
\noun default
 class is responsible for maintaining configuration parameters of the experience
 including calibration for multi-view scenarios and flags for whether bodies
 should be isolated from the scene and whether skeletons should be shown
 in the server display.
 Some of these, including the positions of calibration markers must be known
 by clients and as such the 
\noun on
KinectServer
\noun default
 is capable of delivering the current settings to each client.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
settings window figure?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
When considering the transition from a single stream scenario to the multi-sourc
e product, it can be seen that the settings should no longer be global.
 Taking the calibration settings as an example, the positions and ID's of
 each marker refer to those visible within a scene.
 These will have no meaning if made central parameters, they are closely
 tied to the scene and therefore the source.
 Each source should have it's own independent set of settings and the server
 should be able to deliver the settings to the corresponding clients.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
The 
\noun on
SourceCollection
\noun default
 presented a natural location to store each source's settings object alongside
 it's last frame, again aiming to employ the object-oriented ethos of encapsulat
ion.
 This allowed the settings to be governed by the existing logic including
 
\emph on
stale source
\emph default
 identification.
\end_layout

\begin_layout Standard
The global settings object was not removed but instead had it's function
 changed.
 The object was altered to define the default settings on which each newly
 connected source's are based on.
\end_layout

\begin_layout Subsection
Frame Rate Throttling
\begin_inset CommandInset label
LatexCommand label
name "subsec:Frame-Rate-Throttling"

\end_inset


\end_layout

\begin_layout Standard
As discussed in the evaluations for the 
\noun on
LiveScan
\noun default
 suite (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Evaluation"
plural "false"
caps "false"
noprefix "false"

\end_inset

), the inclusion of buffers can add to the total effective delay of frames
 from capture to presentation as they propagate through each.
 This effect is exacerbated when operating across long distances such as
 between cloud locations.
\end_layout

\begin_layout Standard
A proposed hypothesis for reducing this delay was to limit the transmitted
 frame rate.
 It follow that this would reduce the population of each buffer and reduce
 the time that each frame spent within each.
\end_layout

\begin_layout Standard
This was implemented based on further work by Selinis introducing the concept
 of a dynamic step calculated from a provided FPS and latency requirement
 representing the percentage of frames dropped by the client.
 This step is transmitted in the header of each frame request of a client
 theoretically allowing the system to dynamically respond to changes in
 network conditions.
 This step relied on global statistics calculated about the 
\noun on
KinectServer
\noun default
 including FPS and network bandwidth.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
When considering the final applications of these concepts in the context
 of a multi-source project, the ability to have a step per source was included
 in order to take into account the different network conditions between
 the server and each source's client(s).
 A single set of global FPS and delay requirements would still be provided
 and a step for each source would be calculated and distributed.
\end_layout

\begin_layout Standard
In order to implement these concepts and evaluate the posed hypothesis the
 following was required,
\end_layout

\begin_layout Itemize
Extend the provided stats calculation function to do so for each source
 as opposed to globally.
\end_layout

\begin_deeper
\begin_layout Itemize
This allows source-specific dynamic steps to be used.
\end_layout

\end_deeper
\begin_layout Itemize
Implement a moving average scheme on successive subsets of latencies from
 client to server.
\end_layout

\begin_layout Itemize
Allow either an FPS or delay requirement to be provided as well as a combination
 of the two in order to the prioritisation of either.
\end_layout

\begin_layout Subsubsection
Multi-Source Stats
\end_layout

\begin_layout Standard
The existing calculations of live statistics provided the global FPS and
 network bandwidth of the 
\noun on
KinectServer
\noun default
.
 In order to collect these values on a source-by-source basis a dictionary
 of 
\noun on
SourceStat
\noun default
 objects were used.
 This object collects the FPS, bandwidth and average latencies for each
 source allowing simple retrieval and update when iterating through each
 socket.
\end_layout

\begin_layout Standard
In implementing the latency calculations both a simple moving average or
 traditional mean and exponential moving average were taken in order to
 investigate the efficacy of both.
 The equation for exponential moving average can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Exponential-moving-average"
plural "false"
caps "false"
noprefix "false"

\end_inset

, this iterative version was well suited to processing each element of latency.
 Initially the values were found by collecting the latency of each frame
 currently within the receiving buffers.
 Theoretically this would have allowed the calculated average to be controlled
 by the speed with which frames are moved through the buffer.
 However this was found to be ineffective as the buffers typically stay
 empty.
 Instead queue of fixed size was defined in order to retain the most recent
 values of latency as frames were received by the server.
 This queue included the calculation for both exponential and simple moving
 averages of the contents.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula 
\[
S_{t}\begin{cases}
Y_{1}, & t=1\\
\alpha\cdot Y_{t}+\left(1-\alpha\right)\cdot S_{t-1} & t>1
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $\alpha$
\end_inset

 = Weighting factor
\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $Y_{t}$
\end_inset

= Value at time 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $S_{t}$
\end_inset

= EMA Value at time 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Exponential moving average calculation
\begin_inset CommandInset citation
LatexCommand cite
key "ema"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Exponential-moving-average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
step calculation information
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cross-Platform Mobile AR
\end_layout

\begin_layout Section
Testing Methodology
\end_layout

\begin_layout Standard
The following outlines the methods used to investigate the effect of limiting
 client transmission frame rates in order to control the effective display
 latency (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Subsection
Premise Validation
\end_layout

\begin_layout Standard
Development was conducted with the client and server located on the same
 machine reducing the latency to just that of processing the frames themselves.
 In order to validate the initial premise of the hypothesis investigations
 were made on two machines across a domestic local area network.
\end_layout

\begin_layout Standard
When situated on the same machine the bandwidth used by the suite averages
 around 275Mbps for a full scene.
 When transmitting only bodies these numbers can be more dynamic based on
 the number of and size of figures in the frame.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:body-bandwidth-variation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the variation in network bandwidth for a single body moving closer
 to and further away from the camera, it can be seen to vary between 5 and
 125 Mbps.
 These be taken as the ideal or desired bandwidth requirements both both
 configurations.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/BodyBandwidthVariation.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Variation in used bandwidth for a single body at different distances from
 the 
\noun on
Kinect
\noun default
 sensor
\begin_inset CommandInset label
LatexCommand label
name "fig:body-bandwidth-variation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The small scale of the controlled LAN environment does not inherently incur
 long latencies between nodes and as such does not naturally present a valid
 environment for these investigations.
 To test such a scenario full-scene transmissions were used in order to
 saturate the available bandwidth.
 The connection between nodes included a WiFi connection which limited the
 available bandwidth to around 100Mbps.
 As the full-scene configuration requires around 275Mbps, artificial latency
 was introduced as frames take longer to deliver.
\end_layout

\begin_layout Standard
The step was specified and varied manually initially in order to visualise
 the response in delay and FPS.
 Following implementation of a dynamic step, the response was again visualised.
\end_layout

\begin_layout Subsection
Natural Environment Evaluations
\end_layout

\begin_layout Standard
In order to properly evaluate the hypothesis in a more natural environment
 for the suite, the server was migrated to a virtual machine running in
 
\noun on
Microsoft
\noun default
's 
\noun on
Azure
\noun default
 cloud computing environment.
 A 
\emph on
Standard F2s V2
\emph default
 class virtual machine was used, the specifications for which can be seen
 in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Virtual-Machine-Specifications"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
When operating over the open internet the domestic internet connection became
 relevant as a limiting factor, specifically the rated upload speed of 10Mbps.
 With respect to the desired speeds mentioned above (5 - 275 Mbps), this
 can be seen to be a significant bottleneck.
 As such figure-only capture was used in order to limit the required bandwidth.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
LAN Premise Validation
\end_layout

\begin_layout Standard
Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 present two test scenarios with a manually defined and varied client frame
 drop rate.
 Latency EMA refers to the moving exponential average of latency values
 received from connected clients.
 An alpha of 0.5 was used for both scenarios.
 Both scenarios show an initial spike in latency despite figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 having a 50% drop rate from 
\begin_inset Formula $t=0$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/LatencyStep2.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSStep2.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Latency and FPS response for a manually varied frame drop step (Full-scene
 capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:Latency-and-FPS1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Generally the lowering of the drop rate induces the latency to begin increasing
 at a fairly linear rate.
 This can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 at times 
\begin_inset Formula $t=133$
\end_inset

 (Increasing at roughly 
\begin_inset Formula $176\,\unitfrac{ms}{s}$
\end_inset

) and 
\begin_inset Formula $t=250$
\end_inset

 (
\begin_inset Formula $\approx151\,\unitfrac{ms}{s}$
\end_inset

) and in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 at times 
\begin_inset Formula $t=25$
\end_inset

 (
\begin_inset Formula $\approx304\,\unitfrac{ms}{s}$
\end_inset

) and 
\begin_inset Formula $t=270$
\end_inset

 (
\begin_inset Formula $\approx315\,\unitfrac{ms}{s}$
\end_inset

).
 Conversely increasing the step beyond a critical value causes a peak in
 the latency before inducing the value to begin dropping.
 There is a noticeable lag in response, changes in latency are offset in
 time from the reception of a changed drop rate.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/LatencyStep1.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSStep1.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Latency and FPS response for a manually varied frame drop step (Full-scene
 capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:Latency-and-FPS2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Looking to the FPS data, in both test scenarios the frame rate drops in
 response to an increase in frame drop rate and vice versa.
 Similarly to latency a lag can be seen when changes in drop rate are received.
\end_layout

\begin_layout Subsection
Cloud Environment
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:raw-cloud-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the average transmission latency with an uncontrolled frame drop
 rate.
 The used bandwidth is also visualised averaging around 10Mbps, the rated
 upload speed for the domestic internet connection.
 The latency can be seen to increase linearly and constantly with the exponentia
l and simple moving averages increasing at 
\begin_inset Formula $500\,\unitfrac{ms}{s}$
\end_inset

 and 
\begin_inset Formula $460\,\unitfrac{ms}{s}$
\end_inset

 respectively.
 The exponential moving average can be seen to move faster in response to
 new values, a feature also seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:unstable-cloud-based-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/RawLatencyWithoutStep.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Average transmission latency with no client frame drop (Figure-only capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:raw-cloud-latency"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:unstable-cloud-based-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

 highlights the instability encountered in the cloud based environment.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/CloudInstabilityLatency.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/CloudInstabilityBandwidth.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cloud based latency and bandwidth as a function of a manual frame drop step
 (Figure-only capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:unstable-cloud-based-latency"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Subsection
LAN Premise Validation
\end_layout

\begin_layout Standard
The initial spike in latency could be a result of the delay in drop rate
 delivery from server to client allowing the client's buffer to initially
 fill at an uncontrolled rate.
\end_layout

\begin_layout Standard
The response lag could be attributed to a couple of influencing factors.
 The nature of a moving average partially representing previous values when
 iterating adds a 
\emph on
friction
\emph default
 to each successive result and an inherent lag when the data swings.
\end_layout

\begin_layout Standard
Additionally, the drop rate was employed at the client prior to queueing
 frames in the transmission buffer.
 Frames are then delivered to the server from this buffer decoupled from
 this rate.
 As a result when a new drop rate is received the buffer is filled at this
 rate going forward however it was likely not empty and the server must
 still receive the existing population queued at a different rate.
\end_layout

\begin_layout Subsection
Cloud Environment
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Section
Future Work
\end_layout

\begin_layout Standard
Below outlines possible future developments to the 
\noun on
LiveScan3D
\noun default
 suite with regards to both the work presented in this project and the suite
 as a whole.
\end_layout

\begin_layout Standard
The migration of the mobile application's AR environment in 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 to that of 
\noun on
Unity
\noun default
's native 
\noun on
ARFoundation
\noun default
 framework allowed deploying to both iOS and Android phones.
 However, the previously used 
\noun on
GSBillboard
\noun default
 shader is not compatible with the iOS 
\noun on
Metal
\noun default
 graphics library due to it being a geometry shader, limiting functionality
 to just that of the network layer.
 With a compatible shader the cross-platform migration would be feature-complete
, extending the possible install base of the application while also providing
 access to the currently wider feature set of iOS's 
\noun on
ARKit
\noun default
.
\end_layout

\begin_layout Standard
With investigations being made into the speed and latency of network communicati
ons, this is an area with much space for development.
 The use of the TCP protocol for frame delivery includes overhead compared
 to the more standard UDP for interactive media applications.
 Migrating to UDP could present opportunities to increase the network speed.
\end_layout

\begin_layout Standard
There are many ways in which the packet size could be reduced, one already
 supported by the suite is the use of 
\noun on
ZStandard
\noun default
 compression, a lossless compression algorithm developed at 
\noun on
Facebook
\noun default
.
 A similar method could be the use of hologram sub-sampling, particularly
 between the server and user experience.
 This would involve reducing the size of a transmitted holograms by sampling
 only a percentage of the point cloud vertices and increasing the point
 size at display to cover the space inbetween.
 When presented at a small scale at the user experience this could present
 a method to reduce transmission bandwidth while maintaining much of the
 final experience.
\end_layout

\begin_layout Standard
Another method for achieving this reduction in bandwidth could be inherited
 from traditional video compression through the use of 
\emph on
frame types
\emph default
 including I-frames, P-frames and B-frames.
 Essentially when capturing a whole scene, it is unlikely that all of it
 will change significantly each frame and as such calculating and transmitting
 only the changes since the last could significantly reduce the required
 bandwidth.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
UDP network?
\end_layout

\begin_layout Plain Layout
Fix iOS shaders
\end_layout

\begin_layout Plain Layout
better step calculation
\end_layout

\begin_layout Plain Layout
re-enabling compression
\end_layout

\begin_layout Plain Layout
sampling holograms at client before transmission, increase point size?
\end_layout

\begin_layout Plain Layout
Maybe at server for UE?
\end_layout

\begin_layout Plain Layout
IBP frames
\end_layout

\end_inset


\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
Within this piece the process of extending the 
\noun on
LiveScan3D
\noun default
 software to include multi-source holoportation has been presented.
 The use of such a system has many applications from those inherited from
 traditional 2D video such as conference calls to wholly unique applications
 within the new domain.
\end_layout

\begin_layout Standard
The literature review contextualises the 
\noun on
LiveScan
\noun default
 suite within the wider spaces of XR, 3D video and multi-source holoportation
 itself.
 Previous examples of holoportation are presented and their aims of achieving
 telepresence are discussed.
\end_layout

\begin_layout Standard
The results of the project are 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
laid out showing good progress through the required areas of development
\end_layout

\end_inset

.
 Of these areas of concern, the display element has been extended in order
 to allow the rendering of multiple environments simultaneously with a dynamic
 sub-system of geometric transformations.
 The transformations are responsive to user input allowing arbitrary placement
 and orientation of individual sources within the display space.
 While this control interface allows free movement in the most naturally
 traversed axes it could use some additional tuning to make it feel more
 intuitive.
\end_layout

\begin_layout Standard
The next steps for the project leading up to its completion are presented,
 the initial and current plans for the remaining work is described and additiona
l stretch goals are defined for any additional time.
 How the work will be presented in a final report is also described.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
Holoportation and multi-source configurations thereof are important technologies
 for cross reality experiences with broad appeal to many applications.
 The use of consumer hardware, specifically the 
\noun on
Kinect
\noun default
, has accelerated the space.
\end_layout

\begin_layout Standard
At roughly halfway through the time allowed for this project the native
 display has successfully been extended to meet the deliverable specification.
 This has resulted in the 
\noun on
OpenGL
\noun default
 window being capable of simultaneously rendering multiple sources with
 arbitrary placement and orientation within the display space.
\end_layout

\begin_layout Standard
From this point the network layer of the suite will be developed to also
 match the specification, allowing connected clients to be grouped into
 sources for polling and processing.
\end_layout

\begin_layout Standard
Following the development of the two, testing methodologies will be defined
 and carried out to gather quantitative results for the final product.
 A final report on the results will be available in May 2020.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "/home/andy/uni/dissertation/references"
options "bibtotoc"

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Server UI Additions
\end_layout

\begin_layout Standard
During the development of the network behaviour of the suite, various UI
 elements were added to the server application in order to aid in debugging.
 The final UI can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LiveScan-server-UI"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ServerWindow.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
LiveScan server window following the addition of UI elements for viewing
 buffer capacity, network bandwidth and operating frequencies
\begin_inset CommandInset label
LatexCommand label
name "fig:LiveScan-server-UI"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
add before picture
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Receiving and transmitting network bandwidth 
\end_layout

\begin_layout Itemize
Buffer usage
\end_layout

\begin_layout Itemize
Operating frequencies
\end_layout

\begin_layout Itemize
Connected sources
\end_layout

\begin_layout Itemize
Connected user experiences
\end_layout

\begin_layout Section
Network Message Types
\begin_inset CommandInset label
LatexCommand label
name "sec:Network-Message-Types"

\end_inset


\end_layout

\begin_layout Itemize
MSG_CAPTURE_FRAME
\end_layout

\begin_layout Itemize
MSG_CALIBRATE
\end_layout

\begin_layout Itemize
MSG_RECEIVE_SETTINGS
\end_layout

\begin_layout Itemize
MSG_REQUEST_STORED_FRAME
\end_layout

\begin_layout Itemize
MSG_REQUEST_LAST_FRAME
\end_layout

\begin_layout Itemize
MSG_RECEIVE_CALIBRATION
\end_layout

\begin_layout Itemize
MSG_CLEAR_STORED_FRAMES
\end_layout

\begin_layout Itemize
MSG_CONFIRM_CAPTURED
\end_layout

\begin_layout Itemize
MSG_CONFIRM_CALIBRATED
\end_layout

\begin_layout Itemize
MSG_SEND_STORED_FRAME
\end_layout

\begin_layout Itemize
MSG_SEND_LAST_FRAME
\end_layout

\begin_layout Itemize
MSG_NO_FRAME 
\end_layout

\begin_layout Section
New Data Structures
\end_layout

\begin_layout Subsection
Frame
\begin_inset CommandInset label
LatexCommand label
name "subsec:Frame"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand lstinputlisting
filename "../snippets/frame.cs"
lstparams "language={[Sharp]C},caption={Point cloud with Client ID}"

\end_inset


\end_layout

\begin_layout Section
Virtual Machine Specifications
\begin_inset CommandInset label
LatexCommand label
name "sec:Virtual-Machine-Specifications"

\end_inset


\end_layout

\begin_layout Subsection
Standard F2s v2
\end_layout

\begin_layout Standard
Specifications found at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout
https://docs.microsoft.com/en-us/azure/virtual-machines/fsv2-series
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Based on the Intel Xeon Platinum 8168.
 It features a sustained all core Turbo clock speed of 3.4 GHz and a maximum
 single-core turbo frequency of 3.7 GHz.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vCPUs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Memory 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 GiB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Temp Storage (SSD)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16 GiB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Max Data Disks
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Expected Network Bandwidth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
875 Mbps
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_body
\end_document
