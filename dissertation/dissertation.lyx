#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 
\pagenumbering{roman}
\end_preamble
\use_default_options true
\begin_modules
customHeadersFooters
minimalistic
todonotes
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_title "Multi-Source Holoportation with LiveScan3D"
\pdf_author "Andy Pack"
\pdf_subject "Undergraduate dissertation outlining developments made to extend the LiveScan3D suite to support streaming multiple scenes simultaneously"
\pdf_keywords "LiveScan3D, Kinect, Holoportation, AR"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plain
\biblio_options urldate=long
\biblatex_bibstyle ieee
\biblatex_citestyle ieee
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.8cm
\topmargin 2cm
\rightmargin 1.8cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\bullet 1 0 9 -1
\bullet 2 0 24 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\size giant
Multi-Source Holoportation 
\begin_inset Newline newline
\end_inset

with LiveScan3D
\end_layout

\begin_layout Author
Andy Pack
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../surreylogo.png
	lyxscale 15
	width 18col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 4pheight%
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{changemargin}{3cm}{3cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center

\size large
A dissertation submitted to the Department of Electronic Engineering in
 partial fulfilment of the Degree of Masters of Engineering in Electronic
 Engineering
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{changemargin}
\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
Supervised by Professor Ning Wang
\end_layout

\begin_layout Standard
\noindent
\align center
May 2020
\size large

\begin_inset Newline newline
\end_inset

Department of Electrical and Electronic Engineering
\begin_inset Newline newline
\end_inset

Faculty of Engineering and Physical Sciences
\begin_inset Newline newline
\end_inset

University of Surrey
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Abstract
An extension to the 
\noun on
LiveScan3D
\noun default
 suite is presented, facilitating the acquisition and presentation of multiple
 scenes simultaneously.
 In doing so the application suite already strengthened by its domain-agnostic
 nature can be seen to be further generalised allowing conference call-like
 experiences.
 Developments were made to the server display component to allow this concurrent
 presentation and to the network layer of the suite in order to differentiate
 sources.
 Extensions to the 
\noun on
Android
\noun default
 AR application are presented, both in achieving the multi-source aim and
 in facilitating deployment to iOS devices using Unity's 
\noun on
ARFoundation
\noun default
.
 Efforts are made to define source synchronisation, coarse and fine sync
 methods are described.
 A method of coarse synchronisation is investigated by intentionally throttling
 frame rate in an effort to meet a defined latency requirement.
 This method was shown to successfully constrain the otherwise unbounded
 latency in adverse network conditions, although the limitations of the
 linear implementation is evaluated.
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
\noindent
\align center
I'd like to extend my thanks to Professor Ning Wang for both the learning
 opportunities provided by this project and his continued support.
 
\end_layout

\begin_layout Standard
\noindent
\align center
I would also like to thank Ioannis Selinis and Sweta Anmulwar for their
 shared patience and help throughout this year.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\begin_inset CommandInset toc
LatexCommand lstlistoflistings

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Right Footer
Andy Pack / 6420013
\end_layout

\begin_layout Left Footer
May 2020
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic} 
\end_layout

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The immersive technology spaces of virtual and augmented reality promise
 to change the way we experience digital media.
 Spurred in recent years by the release of consumer VR headsets and the
 push for handheld AR using smartphones, no longer do these experiences
 present merely proofs of concept but complete commercial products.
 
\end_layout

\begin_layout Standard
While some present natural extensions to existing technology as seen in
 VR gaming, others are more tied to the new domain.
 The power of modern smartphones has allowed the integration of augmented
 reality into applications both as the primary function and in more secondary
 features such as visualising products in shopping apps like 
\noun on
IKEA Place
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "ikea-place"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
No matter the application, common to all is the importance of the presented
 media itself.
 Typically this is in the form of pre-recorded meshes of 3D objects, captured
 and stored within the application for both the previously mentioned AR
 and VR examples.
 Less seen in commercial products is the live-streaming of 3D renders due
 in part to both the required network bandwidth and the hardware required
 to capture such media.
\end_layout

\begin_layout Standard
One such technology for the capture and transmission of 3D video is 
\noun on
LiveScan3D
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d"
literal "false"

\end_inset

, a client-server suite of applications utilising the 
\noun on
Microsoft Kinect
\noun default
 camera to capture a scene in real-time and deliver the result to a server
 for reconstruction and presentation.
 Renders or 
\emph on
holograms
\emph default
 can then be delivered to a 
\emph on
user experience
\emph default
 such as an AR or VR client, together a process referred to as 
\emph on
holoportation
\emph default
 (hologram teleportation).
\end_layout

\begin_layout Standard
This project aims to extend this suite to support multi-source holoportation,
 receiving multiple scenes concurrently in a many-to-one configuration ready
 for composite presentation.
 In doing so the implementation of holoportation could be seen to be generalised
, extending the possible applications of the suite.
 
\end_layout

\begin_layout Standard
One application would be support for experiences akin to conference-calls
 with multiple actors capturing their separate environments for composition
 at the server.
 This could be seen to apply both to productivity software similar to existing
 conference-call software and entertainment experiences with multiple locations
 combined for consumption by the public.
 
\end_layout

\begin_layout Standard
As the spaces of augmented and virtual reality become more commonplace and
 mature, the ability to capture and stream 3D renders over the internet
 using consumer-grade hardware has many possible applications and presents
 one of the most direct evolutions of traditional video streaming.
\end_layout

\begin_layout Standard
A conceptual view of a multi-source configuration can be seen in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:premise"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Both single and multi-view sources are shown, the latter allowing more
 complete renders of the subject to be acquired from different angles.
 Both shapes are presented at the 
\emph on
user experience
\emph default
, typically envisaged as an AR or VR client.
\end_layout

\begin_layout Standard
The code for this project resides in two 
\noun on
Github
\noun default
 repositories the URLs for which can be seen below, the reports and associated
 materials including gathered data is also under source control.
\end_layout

\begin_layout Standard
\noindent
\align center

\noun on
LiveScan3D
\noun default
 Suite: 
\begin_inset CommandInset href
LatexCommand href
name "github.com/Sarsoo/LiveScan3D"
target "https://github.com/Sarsoo/LiveScan3D"
literal "false"

\end_inset


\begin_inset Newline newline
\end_inset

Mobile AR Application: 
\begin_inset CommandInset href
LatexCommand href
name "github.com/Sarsoo/LiveScan3D-Unity"
target "https://github.com/Sarsoo/LiveScan3D-Unity"
literal "false"

\end_inset


\begin_inset Newline newline
\end_inset

Reports and Project Materials: 
\begin_inset CommandInset href
LatexCommand href
name "github.com/Sarsoo/dissertation"
target "https://github.com/Sarsoo/dissertation"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/premise.png
	lyxscale 30
	width 45col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Demonstration of a multi-source holoportation system including single and
 multiple view camera configurations
\begin_inset CommandInset label
LatexCommand label
name "fig:premise"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Objectives
\end_layout

\begin_layout Standard
In order to achieve the goal of multi-source holoportation the following
 key objectives must be achieved,
\end_layout

\begin_layout Enumerate
Introduce a source ID in network communications to identify each frame of
 a transmitted hologram.
 This includes redefining much the packet structure in order to include
 this ID between client-server and server-UE communications.
\end_layout

\begin_layout Enumerate
Migrate the server from a single-source streaming node to a manager of multiple
 scenes with a method to temporally synchronise sources.
 This also includes defining methods of identifying sources that are no
 longer actively transmitting frames and to redefine the settings to be
 per-source.
\end_layout

\begin_layout Enumerate
Extend the native display viewfinder of the server in order to separately
 render each connected source.
 Allow individual placement of holograms using keyboard controls.
 Include automatically coherent placement of newly connected sources.
\end_layout

\begin_layout Enumerate
Update the 
\noun on
Android
\noun default
 AR application to separately present each source of footage and allow individua
l control.
 This would represent multi-source developments to an AR presentation layer.
\end_layout

\begin_layout Standard
These can be seen contextually highlighted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:High-level-objectives"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ObjectivesSummary.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
High-level diagram of the suite with key objectives highlighted
\begin_inset CommandInset label
LatexCommand label
name "fig:High-level-objectives"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Challenges
\end_layout

\begin_layout Paragraph
Connection Quality
\end_layout

\begin_layout Standard
Introducing multiple sources to the experiences increases complexity in
 many ways.
 When assuming geographically distant sources it could also be assumed there
 will be different network conditions between each and the server.
 How the server handles and presents differences in connection quality will
 be critical to the user experience when consumed in AR or VR where such
 discrepancies would otherwise be clear and harmful.
 Methods to identify sources no longer actively transmitting frames are
 presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stale-Source-Culling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Bandwidth
\end_layout

\begin_layout Standard
The bandwidth for multiple sources can be expected to scale linearly in
 the same way as multi-view configurations, with regards to network requirements
 the differences between these two configurations are by classification
 only.
 For an already demanding service, this could require additional management.
 An expected environment for the server would be a cloud-based environment
 where bandwidth can be expected to be high, investigations into it's behaviour
 in this scenario are presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Cloud-Results"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Temporal Synchronisation
\end_layout

\begin_layout Standard
Synchronising sources for display poses the largest challenge to a seamless
 experience for the user.
 Although some applications could tolerate a time offset in display, this
 could break the immersion for many others.
 The previous conference call example would be one where this offset would
 not be tolerable, the audio delay incurred during transatlantic phone calls
 can make communication harder in the same way.
 Additionally it is likely that such a system would need to be situated
 in the main processing flow of frames at the server, as such a high performance
 implementation must be found in order not to detriment the overall performance.
 Two theories of synchronisation are presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Source-Synchronisation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and the efficacy of one implementation is investigated in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
COVID-19
\begin_inset CommandInset label
LatexCommand label
name "subsec:COVID-19"

\end_inset


\end_layout

\begin_layout Standard
Conducted throughout the 2019/20 academic year, the project was inevitably
 affected by the global COVID-19 pandemic.
 With a lack of access to the on-campus lab environment, from March onwards
 there was access to only a single 
\noun on
Kinect
\noun default
 sensor and no access to an 
\noun on
Android
\noun default
 phone with AR support.
 This significantly hindered the ability to evaluate the implemented multi-sourc
e capabilities and some elements of the mobile AR application.
 As a result, two alternative objectives were identified to allow quantitative
 analysis.
\end_layout

\begin_layout Enumerate
Migrate the AR mobile application to allow deployment on both 
\noun on
Android
\noun default
 and iOS devices.
 This would facilitate continued development on available hardware without
 access to the lab.
\end_layout

\begin_layout Enumerate
Implement a dynamic system of frame rate throttling to investigate the effect
 on effective display latency.
 This would implement a method of source synchronisation and support additional
 research being conducted on the network behaviour of the 
\noun on
LiveScan
\noun default
 suite.
\end_layout

\begin_layout Paragraph
Challenges
\end_layout

\begin_layout Standard
The biggest challenge to the mobile AR migration is the scope of changes
 being made to the codebase, changing the base library used to create and
 manage the AR environment.
 This is a significant change and poses many opportunities to cause issues
 compounded by the goal of building for a different device than initially
 intended implying a different build toolset.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
frame rate challenge?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Key Technical Achievements
\end_layout

\begin_layout Standard
This section summarises the design and implementation of the developments
 made within this project.
\end_layout

\begin_layout Subsubsection
Multi-source 
\noun on
LiveScan
\end_layout

\begin_layout Standard
Developing the server OpenGL display involved the creation of a subsystem
 of dynamic geometric transformations in order to arrange holograms in the
 3D space.
\end_layout

\begin_layout Standard
The 
\noun on
Transformer
\noun default
 class provides static methods to apply affine transformations to coordinates
 using matrix arithmetic.
 Transformations can also be compounded for efficient application to many
 vertices.
 Utility functions allow the retrieval of rotation matrices in all three
 axes using Euler angles.
\end_layout

\begin_layout Standard
This utility class is used extensively by the 
\noun on
DisplayFrameTransfomer
\noun default
, the object responsible for maintaining the spatial state of holograms
 within the display space.
 Each frame, the display retrieves the location of each source and applies
 this transformation to holograms in their own coordinate space.
 Keyboard controls trigger the selected hologram to move, these actions
 are passed to the 
\noun on
DisplayFrameTransfomer
\noun default
 to update a source's location.
\end_layout

\begin_layout Standard
An integer source ID identifies frames of a hologram when transmitted both
 from client to server and server to user experience.
 The source ID uses a single byte during transmission allowing a theoretical
 128 concurrent sources.
 Communications between the server and a connected user experience only
 pertain to frame transmission, as such the source ID is included in the
 header of the packet.
 A wider array of messages are sent between client and server including
 calibration triggers and settings delivery.
 As many of these have no semantic relation to the source itself the source
 ID instead prepends the payload when transmitting frames.
 In both cases this allows the header structure to stay consistent.
\end_layout

\begin_layout Standard
The concepts of 
\emph on
active
\emph default
 and 
\emph on
passive
\emph default
 source synchronisation are introduced.
 Assuming synchronised timestamps across clients, server and user experiences
 using the included NTP functionality, source synchronisation attempts to
 match frames captured at the same time during display.
 Passive sync acts as a coarse match and aims to keep frames within a tolerable
 latency interval by responding to current environment conditions.
 Within this work, this is exemplified by throttling transmitted frame rate
 to make all sources meet the same latency requirement.
 Active sync acts as a fine control, tightly matching source frames of the
 closest timestamp from within the processing pipeline of the server.
 Active sync was unable to be implemented within this project due to the
 lack of access to multiple 
\noun on
Kinect
\noun default
 sensors however it is theorised that a combination of the two methods will
 provide the tightest synchronisation.
\end_layout

\begin_layout Standard
The mobile AR application was updated in order to allow the simultaneous
 display of multiple holograms.
 Utilising strengths of 
\noun on
Unity
\noun default
 development, this did not require a significant redesign of the written
 code, instead, the hierarchy of game objects and components could be altered.
 The 
\noun on
PointCloudRenderer
\noun default
 component responsible for rendering the hologram from pixel-like primitive
 objects becomes responsible for managing a single source.
 The 
\noun on
PointCloudReceiver
\noun default
 network layer now maintains a dictionary of these display managers indexed
 by source ID.
 The required alterations to the touch inputs were not able to be implemented
 before losing access to the lab environment, this remained to be completed
 at the end of the project.
\end_layout

\begin_layout Standard

\emph on
Stale
\emph default
 source were defined as sources that have not delivered a frame to the server
 within a given time interval.
 This allows them to be removed from live display in order to maintain the
 experience of other connected sources.
 This was implemented through the 
\noun on
SourceCollection
\noun default
 object.
 This object wraps around the dictionary used to store frames and includes
 a thread responsible for checking the timestamp of the last received frame.
\end_layout

\begin_layout Standard
The settings object previously maintained globally was redefined as being
 a source level variable.
 Containing calibration information, this allowed sources to be setup in
 multi-view configurations.
 These settings objects were also stored within the 
\noun on
SourceCollection
\noun default
.
\end_layout

\begin_layout Subsubsection
Cross-platform Mobile AR
\end_layout

\begin_layout Standard
In an effort to continue using the mobile AR application without access
 to an 
\noun on
Android
\noun default
 phone, efforts were made to allow deployment to iOS devices.
 The existing application constructs an AR environment using 
\noun on
Google's
\noun default
 
\noun on
ARCore
\noun default
 library which theoretically includes native support for deploying to iOS.
 However, following issues when attempting to test this configuration, the
 app was instead migrated to use 
\noun on
Unity
\noun default
's native AR framework, 
\noun on
ARFoundation
\noun default
.
\end_layout

\begin_layout Standard

\noun on
ARFoundation
\noun default
 acts as an abstraction layer for 
\noun on
Android
\noun default
 and iOS's native AR libraries, 
\noun on
ARCore
\noun default
 and 
\noun on
ARKit
\noun default
 respectively.
 In doing so, development can take place solely in 
\noun on
Unity
\noun default
 components which are replaced by the OS-specific implementation on deployment.
\end_layout

\begin_layout Standard
On deployment, the app started as expected with native UI elements and visible
 plane discovery.
 Without the need to edit any of the 
\noun on
LiveScan
\noun default
 specific code, the network behaviour was also as expected and the client
 connected to the server without issue.
 At this point it was discovered that the included geometry shader used
 by the 
\noun on
Android
\noun default
 implementation would not be compatible in this configuration.
 Supported by 
\noun on
OpenGL ES 3
\noun default
 used by 
\noun on
Android
\noun default
, when deploying to iOS the native 
\noun on
Metal
\noun default
 graphics library is used instead which does not support geometry shaders.
 Without the required experience to source or write a compatible shader,
 this presents the current state of the application.
 This proved useful in allowing continued network debugging of the server
 however clearly doesn't constitute a complete migration without a functioning
 presentation layer.
\end_layout

\begin_layout Subsubsection
Frame rate throttling
\end_layout

\begin_layout Standard
In support of further research into the 
\noun on
LiveScan
\noun default
 network behaviour and to implement a form of passive source synchronisation,
 investigations were made into the effect of limiting transmitted frame
 rate on effective display latency.
\end_layout

\begin_layout Standard
Based on a previous design by Selinis, a proportion of frames were dropped
 at the client before queuing for transmission.
 This rate was referred to as a dynamic step and was calculated by the server
 based on current network conditions including latency and frame rate.
 The step was attached to the header of each frame request allowing it to
 move in response to changing conditions.
\end_layout

\begin_layout Standard
This system was tested in both a local and cloud-based environment with
 conditions applied in order to properly induce long latency.
 Using full-scene capture in order to maximise the used bandwidth, the effect
 of a manually varied step on both frame rate and latency was demonstrated
 in a LAN environment in order to validate the expected relationships between
 these variables.
 Using figure-only capture, a dynamic step was shown to successfully control
 the otherwise unbounded latency.
 The limitations of this approach is discussed including the observed wide
 oscillations around the requirement as a result of the linear step motion.
\end_layout

\begin_layout Standard
Representative data was not able to be gathered in a cloud-based environment
 as a result of the system's instability and domestic network upload speed.
 Preliminary data in this environment is presented with discussions regarding
 how to make the frame rate drop methods better suited to these adverse
 conditions.
\end_layout

\begin_layout Subsection
Report Structure
\end_layout

\begin_layout Standard
The remaining structure of this report is laid out as follows,
\end_layout

\begin_layout Standard
Section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Literature-Review"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the states of the technology that 
\noun on
LiveScan3D
\noun default
 and this project relates to including volumetric video, VR/AR display and
 holoportation.
\begin_inset Newline newline
\end_inset

The 
\noun on
LiveScan3D
\noun default
 suite is described and evaluated in section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:LiveScan3D"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the original state is outlined and the impact of subsequent developments
 are discussed.
\begin_inset Newline newline
\end_inset

Section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Multi-Source-Developments"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the multi-source developments made to each aspect of the 
\noun on
LiveScan
\noun default
 suite, qualitative evaluations are made.
\begin_inset Newline newline
\end_inset

Section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Cross-Platform-Mobile-AR"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes efforts made to extend the mobile AR application to include iOS
 functionality, the limitations are presented with remaining required developmen
ts outlined.
\begin_inset Newline newline
\end_inset

Section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents investigations made into the effect of throttling transmitted
 frame rate on display latency in both a LAN and cloud environment.
\begin_inset Newline newline
\end_inset

Section 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Future-Work"
plural "false"
caps "false"
noprefix "false"

\end_inset

 outlines additional areas of investigation and possible developments that
 could be made into the 
\noun on
LiveScan
\noun default
 suite based both on the work of this project and the existing code.
\begin_inset Newline newline
\end_inset

Finally, sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Summary"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Conclusions"
plural "false"
caps "false"
noprefix "false"

\end_inset

 summarise and provide conclusions for the project.
\begin_inset Newline newline
\end_inset

Cited works are found in the references 
\begin_inset CommandInset ref
LatexCommand vpageref
reference "sec:bibliography"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
In the appendices, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gantt-Chart"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents a Gantt chart for the project while 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Server-UI-Additions"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes UI additions to the server application.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Network-Message-Types"
plural "false"
caps "false"
noprefix "false"

\end_inset

 provides a list of available message types used within the network communicatio
ns.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the new 
\noun on
Frame
\noun default
 struct.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Linear-Frame-Drop"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes the frame drop rate calculation used in the frame rate/latency
 investigations while 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Dynamic-Step-Test"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the entire dataset from which results were presented.
 Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Virtual-Machine-Specifications"
plural "false"
caps "false"
noprefix "false"

\end_inset

 documents the specifications of the virtual machine used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Literature Review
\begin_inset CommandInset label
LatexCommand label
name "sec:Literature-Review"

\end_inset


\end_layout

\begin_layout Standard

\noun on
LiveScan3D
\noun default
 utilises the 
\noun on
Microsoft Kinect
\noun default
 sensor to capture RGB video with depth information.
 While 
\noun on
Kinect
\noun default
 sensors have proved extremely popular in the computer vision sector, it
 does not represent the only method for such 3D reconstruction, the state
 of free-viewpoint video is investigated before identifying the 
\noun on
Kinect
\noun default
's role in this space.
\end_layout

\begin_layout Standard
The significance of the renders captured and relayed by the 
\noun on
LiveScan
\noun default
 suite is related to the development of technologies able to immersively
 display such video content.
 
\end_layout

\begin_layout Standard
While this has been exemplified mostly through AR with 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-hololens"
literal "false"

\end_inset

's 
\noun on
LiveScan
\noun default
 client for 
\noun on
Microsoft Hololens
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-android"
literal "false"

\end_inset

's extension of this for 
\noun on
Android
\noun default
 phones
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-android"
literal "false"

\end_inset

, the generic collection and transmission of 3D holograms has applicability
 to all forms of XR and as such the state of this space is investigated
 with an emphasis on handheld AR.
\end_layout

\begin_layout Standard
Finally, existing examples of holoportation are presented including those
 displaying multi-source capabilities.
\end_layout

\begin_layout Standard
As the foundation of this project, the 
\noun on
LiveScan3D 
\noun default
suite itself is presented in more depth following this review in order to
 contextualise it both within these investigations and the extension work
 presented herein.
\end_layout

\begin_layout Subsection
Volumetric Video
\end_layout

\begin_layout Standard
Within the space of real-time 3D video capture, the state of the art has
 typically involved a purpose-built and static lab environment with an array
 of digital cameras, possibly with additional depth sensors
\begin_inset CommandInset citation
LatexCommand cite
key "microsoft-mixed-reality-studios,surrey-surface-capture"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
While this remains the case for the highest quality free-viewpoint video
\begin_inset CommandInset citation
LatexCommand cite
key "ftv-tanimoto"
literal "false"

\end_inset

, consumer-grade depth aware camera units have facilitated research and
 applications outside of such environments.
\end_layout

\begin_layout Standard
While capable of high-quality results, these implementations require a tightly
 controlled environment and high-performance hardware.
 As such, it is briefly presented here to contextualise the use of consumer-grad
e depth-aware cameras in the 
\noun on
LiveScan
\noun default
 suite.
\end_layout

\begin_layout Subsubsection
Visual Hull Reconstruction in a Lab Environment
\end_layout

\begin_layout Standard
With a selection of 2D videos from multiple viewpoints, 
\emph on
shape-from-silhouette
\emph default
 describes a method to reconstruct depth information through the application
 of Epipolar geometry
\begin_inset CommandInset citation
LatexCommand cite
key "sfs-over-time,sfs-video-cm"
literal "false"

\end_inset

.
 The result of this is referred to as a visual hull
\begin_inset CommandInset citation
LatexCommand cite
key "visual-hull-laurentini,laurentini-solids-revolution"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
A visual hull defines a 3D representation of an object constructed through
 the volume intersection of multiple 2D silhouettes.
 With knowledge of the relative positions of each viewpoint, corresponding
 silhouettes of an object can be triangulated to form a 3D object.
 In doing so depth information lost in projection to a 2D image can be reconstru
cted.
\end_layout

\begin_layout Standard
With the increasing computational power of computers and GPUs, methods to
 do so in real-time with high frame rates have been demonstrated
\begin_inset CommandInset citation
LatexCommand cite
key "cuda-visual-hull"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
While the visual hull alone can produce coherent 3D renders, further application
s typically use it as one aspect of a multi-faceted capture pipeline in
 order to create a spatially and temporally smooth surface
\begin_inset CommandInset citation
LatexCommand cite
key "microsoft-mixed-reality-studios,surrey-surface-capture,hull-correspondences"
literal "false"

\end_inset

.
 Additional processing techniques such as feature correspondences
\begin_inset CommandInset citation
LatexCommand cite
key "hull-correspondences,surrey-surface-capture"
literal "false"

\end_inset

 have also proved effective.
\end_layout

\begin_layout Standard

\noun on
Microsoft's Mixed Reality Studios
\begin_inset CommandInset citation
LatexCommand cite
key "microsoft-mixed-reality-studios"
literal "false"

\end_inset


\noun default
 employ a combination of all of these techniques with a surface constructed
 from infra-red, 
\emph on
shape-from-silhouette
\emph default
 and RGB information.
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "microsoft-mixed-reality-studios"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "microsoft-mixed-reality-studios"
literal "false"

\end_inset

 highlights the efficacy of this combination of information in identifying
 different elements of a figure benefiting from the strengths of each input
 data type.
\end_layout

\begin_layout Standard
These techniques draw data from tightly calibrated camera viewpoints, however,
 when considering consumer-grade applications this is typically not available.
 The use of infra-red structured light and 
\emph on
time-of-flight
\emph default
 sensors has facilitated self-contained depth aware camera units capable
 of being used for these applications without such tightly controlled environmen
ts.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
High quality meshes can be created by using tightly calibrated, high quality
 cameras and complex refinement computations.
 However this tight calibration also presents a limitation of the technique
 in that the accuracy of these inter-camera relationships can limit the
 quality of reconstructed objects.
 This typically limits the wider application of the technique to purpose-built
 lab environments
\begin_inset CommandInset citation
LatexCommand cite
key "microsoft-mixed-reality-studios,surrey-surface-capture"
literal "false"

\end_inset

 and is less mobile.
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
RGB-D Cameras
\end_layout

\begin_layout Standard
While many consumer depth-aware sensors exist including 
\noun on

\begin_inset CommandInset citation
LatexCommand citeauthor
key "structure-sensor"
literal "false"

\end_inset


\noun default
's 
\noun on
Structure Sensor
\begin_inset CommandInset citation
LatexCommand cite
key "structure-sensor"
literal "false"

\end_inset

,
\noun default
 and the rear-facing LIDAR of the 2020 iPad Pro, the most commonly used
 camera in computer vision research is the 
\noun on
Microsoft Kinect
\noun default
 due its availability and price
\noun on
.
\end_layout

\begin_layout Standard
The original iteration used infrared structured light alongside an RGB camera
 to provide colour and depth information about the surroundings.
 The device also included motion tracking and skeleton isolation for figures
 in view.
\end_layout

\begin_layout Standard
Following the release of an SDK for Windows in 2012, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "original-kinect-microsoft"
literal "false"

\end_inset

 at 
\noun on
Microsoft Research
\noun default
 reflects on the original camera's capabilities and the applications to
 computer vision research in 
\begin_inset CommandInset citation
LatexCommand cite
key "original-kinect-microsoft"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Here 3D conference calling of the type described in the introduction without
 AR or VR applications is presented, instead, users watch a composite conference
 space on a screen with all participants rendered within.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Work was undertaken to achieve mutual gaze between participants, a marked
 advantage over traditional conference calls where the lack of such aspects
 of group interaction makes the experience more impersonal.
 Methods of achieving more natural virtual interactions or 
\emph on
telepresence
\emph default
 are covered in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Holoportation-and-Telepresence"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A second version of the camera, v2, was released alongside the 
\noun on
Xbox One
\noun default
 in 2013 and presented many improvements over the original utilising a 
\emph on
time-of-flight
\emph default
 sensor for depth information over the previous structured light.
 A higher-quality RGB camera captures 1080p video at up to 30 frames per
 second with a wider field of view than the original
\begin_inset CommandInset citation
LatexCommand cite
key "kinect-specs"
literal "false"

\end_inset

.
 The physical capabilities of the camera are discussed by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "new-kinect"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "new-kinect"
literal "false"

\end_inset

.
 The second version of the camera was found to gather more accurate depth
 data than the original and was less sensitive to daylight.
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "kinectv1/v2-accuracy-precision"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kinectv1/v2-accuracy-precision"
literal "false"

\end_inset

 found similar results with the v2 achieving higher accuracy results over
 the original.
 The second version did, however, achieve lower precision results than the
 v1 with recommendations made to include pre-processing on acquired depth
 images to control for random noise, 
\emph on
flying pixels
\emph default
 and 
\emph on
multipath interference
\emph default
.
\end_layout

\begin_layout Standard
The 
\noun on
Kinect
\noun default
 is used successfully by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "greenhouse-kinect"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "greenhouse-kinect"
literal "false"

\end_inset

 for object detection in the context of an autonomous vehicle navigating
 a greenhouse.
 The depth information was used in conjunction with the RGB information
 to identify obstacles, while the paper lays out some limitations of the
 camera it was found to be effective in its aim and was capable of running
 on a reasonable computer.
\end_layout

\begin_layout Standard
This second iteration on the 
\noun on
Kinect
\noun default
 is frequently used in computer vision experiments with many of the works
 cited here using it for acquisition.
\end_layout

\begin_layout Subsection
Extended Reality (XR)
\end_layout

\begin_layout Standard
Immersive media experiences enhanced through the use of technology are typically
 defined by the level to which they affect the perception of the user.
 This distinction generally organises technologies into one of three established
 terms, 
\emph on
Virtual Reality
\emph default
, 
\emph on
Augmented Reality
\emph default
 and 
\emph on
Mixed Reality
\emph default
.
\end_layout

\begin_layout Description
Virtual The replacement of a user's experience of unmediated reality, rendering
 a new computer-generated space that the user appears to immersively inhabit.
 Typically achieved through face-mounted headsets (
\emph on
Facebook Oculus, HTC Vive, Playstation VR, Valve Index
\emph default
).
\end_layout

\begin_layout Description
Augmented The enhancement of a user's reality through the overlay of digital
 graphics.
 Typically facilitated with translucent/transparent headsets 
\emph on
(Microsoft Hololens, Google Glass)
\emph default
 or increasingly with 
\begin_inset Quotes eld
\end_inset

Window on the World
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "reality-virtuality-continuum"
literal "false"

\end_inset

 mobile technologies 
\emph on
(Android ARCore
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "ARCore"
literal "false"

\end_inset

, 
\emph on
Apple ARKit
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "arkit"
literal "false"

\end_inset


\emph on
)
\emph default
 facilitating implementations such as 
\emph on
Pokemon GO
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "pokemonGO"
literal "false"

\end_inset

.
\end_layout

\begin_layout Description
Mixed A combination of virtual elements with the real world to facilitate
 interaction with an augmented reality.
 A somewhat broad term owing to its description of a point between augmented
 and virtual reality.
 An emphasis is typically placed on virtual elements existing coherently
 within the real world and interacting in real-time.
\end_layout

\begin_layout Standard
The term 
\emph on
Extended Reality 
\emph default
or XR functions as an umbrella term for all such experiences and is used
 throughout this paper, note that the terms 
\emph on
mediated reality
\emph default
 and *R
\begin_inset CommandInset citation
LatexCommand cite
key "all-reality"
literal "false"

\end_inset

 are also sometimes used where the asterisk refers to 
\begin_inset Quotes eld
\end_inset

all
\begin_inset Quotes erd
\end_inset

 realities.
 
\end_layout

\begin_layout Standard
While individual classes of XR provide ostensibly different experiences
 there is overlap between them, notably all aim to digitally extend a user's
 experience of reality.
 All can also be seen to employ 
\emph on
Spatial Computing
\emph default
 as defined by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "spatial-computing"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "spatial-computing"
literal "false"

\end_inset

 to refer to 
\end_layout

\begin_layout Quote

\emph on
Human interaction with a machine in which the machine retains and manipulates
 referents to real objects and spaces.
\end_layout

\begin_layout Standard
Identifying the common dimensions across XR has led to the proposal of various
 taxonomies providing insights into how each implementation relate to others
\begin_inset CommandInset citation
LatexCommand cite
key "reality-virtuality-continuum,mr-taxonomy,all-reality"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
XR Implementations
\end_layout

\begin_layout Standard
While XR applications have been demonstrated without dedicated hardware
\begin_inset CommandInset citation
LatexCommand cite
key "roomalive"
literal "false"

\end_inset

, the majority utilise dedicated headsets or handheld devices.
\end_layout

\begin_layout Standard
Consumer applications pioneer the spaces of video gaming
\begin_inset CommandInset citation
LatexCommand cite
key "pokemonGO"
literal "false"

\end_inset

, education
\begin_inset CommandInset citation
LatexCommand cite
key "ar-anatomy,ar-education"
literal "false"

\end_inset

 and commerce
\begin_inset CommandInset citation
LatexCommand cite
key "ar-commerce,ikea-place"
literal "false"

\end_inset

 but the use of XR in health-care
\begin_inset CommandInset citation
LatexCommand cite
key "ar-adrenalectomy"
literal "false"

\end_inset

 and dangerous work environments
\begin_inset CommandInset citation
LatexCommand cite
key "ar/vr-construction"
literal "false"

\end_inset

 presents the opportunities for life-saving results.
\end_layout

\begin_layout Standard
An investigation into the value of AR and VR for improving construction
 safety is presented by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "ar/vr-construction"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "ar/vr-construction"
literal "false"

\end_inset

.
 A broad look at the applicability is taken with assessments including VR
 experiences for developing worker balance to aid in working at elevation
 and AR experiences incorporated into the workplace for aiding in task sequencin
g to reduce the effect of memory on safety.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Link with work?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "remixed-reality"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "remixed-reality"
literal "false"

\end_inset

 demonstrate an example of mixed reality through the use of 
\noun on
Kinect
\noun default
 cameras and a virtual reality headset.
 Users are placed in a virtual space constructed from 3D renders of the
 physical environment around the user.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Virtual manipulation of the space can then be achieved with visual, spatial
 and temporal changes supported.
 Objects can be scaled and sculpted in real-time while the environment can
 be paused and rewound.
 The strength of mixed reality comes with the immersion of being virtually
 placed in a version of the physical surroundings, tactile feedback from
 the environment compounds this.
\end_layout

\end_inset

Acquisition is conducted using multiple 
\noun on
Kinect
\noun default
 v2 sensors with the 
\noun on
RoomAliveToolkit
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "roomalive"
literal "false"

\end_inset

 employed for calibration.
 This calibration process utilises a projected series of Gray codes visible
 by each sensor to localise each.
 The 
\noun on
LiveScan
\noun default
 calibration process removes the need for additional projector hardware
 by using a set of printed calibration markers to localise each sensor.
\end_layout

\begin_layout Subsubsection
Handheld Augmented Reality
\begin_inset CommandInset label
LatexCommand label
name "subsec:Handheld-Augmented-Reality"

\end_inset


\end_layout

\begin_layout Standard
This project deals primarily with augmented reality facilitated through
 mobile phones, specifically based on Selinis' work
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-android"
literal "false"

\end_inset

 porting the existing 
\noun on
LiveScan3D Hololens
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

 application to 
\noun on
Android
\noun default
.
 As such, the state of handheld AR development is briefly presented here.
\end_layout

\begin_layout Standard
The advancement of mobile AR bolstered by the growing ubiquity of compatible
 hardware has led it to become a rapidly growing and popular form of XR.
 The introduction of OS-level SDK's in 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "ARCore"
literal "false"

\end_inset

 and 
\noun on
Apple
\noun default
's 
\noun on
ARKit
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "arkit"
literal "false"

\end_inset

 make it easier to create AR experiences without the need to re-implement
 the required boilerplate computer vision concepts.
\end_layout

\begin_layout Standard
These frameworks provide native AR environments in which important prerequisites
 including rear-camera pass-through, device motion tracking and plane tracking
 are implemented with the performance expected of an OS-level library.
\end_layout

\begin_layout Standard
While each of these frameworks can be used directly within a traditional
 app development environment, the 
\noun on
Unity
\noun default
 3D game engine
\begin_inset CommandInset citation
LatexCommand cite
key "unity"
literal "false"

\end_inset

 provides many tools to make AR development easier.
 
\noun on
Microsoft
\noun default
 recommends 
\noun on
Unity
\noun default
 for use with the 
\noun on
Hololens
\noun default
 and this was the toolset used by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-hololens"
literal "false"

\end_inset

 in creating the LiveScan3D companion
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
One of 
\noun on
Unity
\noun default
's greatest strengths lies in its native cross-platform deployment allowing
 building for the largest PC, mobile and games console platforms.
 This in conjunction with 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 
\noun on
Unity
\noun default
 package facilitated Selinis' migration of the 
\noun on
Hololens
\noun default
 application to 
\noun on
Android
\noun default
.
\end_layout

\begin_layout Standard
While 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 
\noun on
Unity
\noun default
 package provides functionality for both 
\noun on
Android
\noun default
 and iOS devices, 
\noun on
Unity
\noun default
 provides a native solution to AR development in its 
\noun on
ARFoundation
\noun default
 framework
\begin_inset CommandInset citation
LatexCommand cite
key "arfoundation"
literal "false"

\end_inset

.
 
\noun on
ARFoundation
\noun default
 abstracts away much of the common behaviour provided by each OS-specific
 library and allows each to provide this functionality at runtime - 
\noun on
ARKit
\noun default
 for iOS, 
\noun on
ARCore
\noun default
 for 
\noun on
Android
\noun default
 etc.
\end_layout

\begin_layout Standard

\noun on
ARFoundation
\noun default
 was used in this project to broaden the compatible devices of the 
\noun on
Android
\noun default
 targeted application and allow deploying to iOS.
\end_layout

\begin_layout Subsection
Holoportation and Telepresence
\begin_inset CommandInset label
LatexCommand label
name "subsec:Holoportation-and-Telepresence"

\end_inset


\end_layout

\begin_layout Standard
The term Holoportation is defined and exemplified in a 
\noun on
Microsoft Research
\noun default
 paper by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "holoportation"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "holoportation"
literal "false"

\end_inset

 where an end-to-end pipeline is laid out for the acquisition, transmission
 and display of 3D video facilitating real-time AR and VR experiences.
 The 
\noun on
Microsoft Research
\noun default
 paper builds on works including by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset

 2 years earlier which describes attempts at achieving 
\begin_inset Quotes eld
\end_inset


\emph on
telepresence
\emph default

\begin_inset Quotes erd
\end_inset

, a term coined by Marvin Minksy to describe the transparent and intuitive
 remote control of robot arms as if they were the controllers own
\begin_inset CommandInset citation
LatexCommand cite
key "marvin-minksy"
literal "false"

\end_inset

.
 The term was broadened by Bill Buxton
\begin_inset CommandInset citation
LatexCommand cite
key "buxton-telepresence"
literal "false"

\end_inset

 to include the space of telecommunications to describe technology being
 used to make someone feel present in a different environment.
 
\end_layout

\begin_layout Standard
In the context of holoportation, this is through the use of 3D video reconstruct
ion.
 The aforementioned work by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset

 used 10 
\noun on
Kinect
\noun default
 cameras to capture a room before virtually reconstructing the models.
 
\end_layout

\begin_layout Standard
In service of demonstrating it's applicability to achieving 
\emph on
telepresence
\emph default
, a figure was isolated from the surroundings and stereoscopically rear-projecte
d onto a screen for a single participant, a result of this can be seen in
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:stereoscopic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/telepresence-stereoscopic.png
	lyxscale 30
	width 30col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of stereoscopic projection of depth aware footage captured by
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Immersive-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:stereoscopic"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\noun on
Microsoft Research
\noun default
 paper demonstrates a system using 8 cameras surrounding a space.
 Each camera captured both stereo near infra-red and monocular colour images
 with additional structured light information to construct a colour-depth
 video stream, a more complex camera configuration than many of the others
 cited.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "velt"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "velt"
literal "false"

\end_inset

 demonstrates a similar holoportation experience to 
\noun on
LiveScan3D
\noun default
 capable of supporting multi-view configurations, it also supports both
 point clouds and meshes.
 Calibrating multiple viewpoints is completed using the extrinsics and intrinsic
s of the camera.
 The extrinsics are the relative positions of each 
\noun on
Kinect
\noun default
 camera while the intrinsics describe the internal properties of each, namely
 the focal length and optical centre.
 
\end_layout

\begin_layout Standard
The intrinsics of the 
\noun on
Kinect
\noun default
 camera can be retrieved from the 
\noun on
Kinect
\noun default
 SDK while the extrinsics are obtained in one of two ways.
 Extrinsics can be imported and parsed from XML for manual selection or
 estimated using 
\noun on
OpenCV
\noun default
 and a checkerboard pattern.
 When considering holoportation systems of this kind, comparatively few
 implement multiple views as a result the increased complexity involved
 in calibration.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Link to livescan?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multi-Source Holoportation
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
More?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The space of multi-source holoportation has been explored by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

 in the context of shared architectural design spaces in virtual reality
 similar to a conference call.
 Two groups of people were captured in 3D using clusters of 
\noun on
Kinect
\noun default
 cameras before having these renders transmitted to the other group.
 Each group reconstructs the other's render for display in virtual reality
 in conjunction with their own.
 In doing so a shared virtual space for the two groups has been created
 and it can be seen to implement the process of holoportation.
 The strength of the system as a shared architectural design experience
 is emergent of the semantics of the virtual space where a World in Miniature
 (WIM) metaphor is used.
\end_layout

\begin_layout Standard
The Worlds in Miniature is described by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "wim"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "wim"
literal "false"

\end_inset

 as a set of interfaces between the user and the virtual space they experience
 using tactile and visual tools.
 The interface involves providing the user with a miniature render of the
 world they are inhabiting that can be interacted with in order to affect
 the full-scale environment around them.
\end_layout

\begin_layout Standard
This navigation tool maps well to 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset

's
\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

 architecture groupware design, an image captured during the work can be
 seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:World-in-Miniature-group-by-group"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/group-by-group.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
World in Miniature render demonstrated in a multi-source holoportation context
 by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:World-in-Miniature-group-by-group"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In comparison to 
\noun on
LiveScan
\noun default
 these provide domain-specific applications, the implementation developed
 within this project aims to provide a general many-to-one application of
 the concept to suit the existing philosophy of the suite.
\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Standard
Current methods to capture volumetric video in both the consumer and professiona
l spaces have been presented.
 The strengths of consumer-grade RGB-D cameras, specifically the 
\noun on
Microsoft Kinect
\noun default
 used by 
\noun on
LiveScan3D
\noun default
, were highlighted.
\end_layout

\begin_layout Standard
The main forms of extended reality were described with the common factors
 across all presented.
 XR's applicability beyond standard media consumption was also briefly highlight
ed.
 
\noun on
LiveScan3D
\noun default
 has two associated AR clients of which the most relevant to this project
 is the mobile AR client, subsequently, the current SDKs facilitating AR
 development were presented and their relation to the 
\noun on
Unity
\noun default
 game engine were described.
 The 
\noun on
Unity
\noun default
 
\noun on
ARFoundation
\noun default
 framework is used in this project to abstract away the device-specific
 AR libraries and extend compatibility to iOS.
\end_layout

\begin_layout Standard
Finally, previous implementations of holoportation were given while the
 concept of 
\emph on
telepresence
\emph default
 was presented.
 Domain-specific examples of multi-source holoportation were described to
 contextualise this projects aim to produce a general implementation.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
LiveScan3D
\begin_inset CommandInset label
LatexCommand label
name "sec:LiveScan3D"

\end_inset


\end_layout

\begin_layout Standard

\noun on
LiveScan3D
\noun default
 is a suite of software developed by Marek Kowalski, Jacek Naruniec and
 Michal Daniluk of the Warsaw University of Technology in 2015
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d"
literal "false"

\end_inset

.
 The suite utilises the 
\noun on
Xbox Kinect
\noun default
 v2 camera to record and transmit 3D renders over an IP network.
 A server can manage multiple clients simultaneously in order to facilitate
 multi-view configurations, it is then responsible for displaying the renderings
 in real-time and/or transmitting holograms to a user experience or UE.
 This architecture can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LiveScanArchitecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
These renderings take the form of a point cloud, a collection of 3D co-ordinates
 each with an associated RGB colour value.
 There are many methods by which point clouds can be used to construct surfaces
 suited for traditional computer graphics applications
\begin_inset CommandInset citation
LatexCommand cite
key "point-cloud-surface"
literal "false"

\end_inset

.
 For an interactive or real-time application, however, the plotting of each
 point of the cloud in a 3D space using a suitable point size can create
 a coloured mesh visually representing the captured object while keeping
 the processing pipeline streamlined.
 This is the approach taken by 
\noun on
LiveScan
\noun default
3D and allows it to function on reasonably powerful hardware.
\end_layout

\begin_layout Standard
As a result of it's analogous nature to a traditional frame of 2D video,
 the terms 
\begin_inset Quotes eld
\end_inset

render
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

point cloud
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

frame
\begin_inset Quotes erd
\end_inset

 are used interchangeably from here.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/LiveScanArchitecture.png
	lyxscale 50
	width 65col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture of the 
\noun on
LiveScan3D
\noun default
 suite
\begin_inset CommandInset label
LatexCommand label
name "fig:LiveScanArchitecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 Client
\end_layout

\begin_layout Standard
The 
\noun on
LiveScan
\noun default
 Client is responsible for interfacing with the 
\noun on
Kinect
\noun default
 sensor via the 
\noun on
Kinect
\noun default
 v2 SDK and transmitting hologram frames to the 
\noun on
LiveScan
\noun default
 Server.
 Body detection takes place client-side, as does calibration when using
 multiple sensors.
\end_layout

\begin_layout Standard
Only one 
\noun on
Kinect
\noun default
 sensor can be connected to each computer as a result of the SDK's restrictions.
 A system used by multiple clients in this way lends itself well to multi-source
 configurations over the internet.
\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 Server
\end_layout

\begin_layout Standard
The server component of the 
\noun on
LiveScan
\noun default
 suite is responsible for managing and receiving 3D renders from connected
 clients.
 These holograms are reconstructed in an interactive 
\noun on
OpenGL 
\noun default
window functioning in a similar fashion to that of a traditional camera
 viewfinder allowing live previews of received data.
 Holograms can then be transmitted to the user experience or UE, constituting
 an XR client such as the 
\noun on
Hololens
\noun default
 or 
\noun on
Android
\noun default
 app.
 
\end_layout

\begin_layout Standard
A hologram frame as defined at the server is made up of the following,
\end_layout

\begin_layout Itemize
A list of 3D vertices
\end_layout

\begin_layout Itemize
A list of associated RGB values for each vertex
\end_layout

\begin_layout Itemize
A list of camera poses defining each 
\noun on
Kinect
\noun default
 sensors position and orientation
\end_layout

\begin_layout Itemize
A list of bodies, skeletons as identified by the 
\noun on
Kinect
\noun default
 sensor
\end_layout

\begin_layout Standard
The existing codebase shared variables for each between the main window
 and the 
\noun on
OpenGL
\noun default
 window, clearing and updating each with received live frames.
\end_layout

\begin_layout Standard
The structure up to reconstruction at the server can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:server-structure"
plural "false"
caps "false"
noprefix "false"

\end_inset

, aspects related to the transmission to user experiences are omitted.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/initial-state.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial architecture of the 
\noun on
LiveScan3D
\noun default
 server
\begin_inset CommandInset label
LatexCommand label
name "fig:server-structure"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Calibration & Multi-View Configurations
\end_layout

\begin_layout Standard
Single view configurations provide one viewing angle of a captured scene,
 this results in areas of un-captured space causing visible shadows in the
 hologram.
 Capturing multiple angles of the same scene allows a more complete composite
 hologram to be constructed and presented.
\end_layout

\begin_layout Standard
When using a single client setup, frames are transmitted in their own coordinate
 space with the origin defined as the 
\noun on
Kinect
\noun default
 camera and the captured scene rendered in front of it.
\end_layout

\begin_layout Standard
When using multiple sensors, the server would be unable to combine these
 unique Euclidean spaces without knowledge of the positions of the sensors
 relative to each other, the 
\emph on
extrinsics
\emph default
 of the system.
\end_layout

\begin_layout Standard
In order to make a composite frame, a calibration process is completed client-si
de following instruction by the server.
\end_layout

\begin_layout Standard
Calibration is completed in two steps, an initial estimation followed by
 a refinement process.
 The initial estimation is completed by informing the server of which calibratio
n marker layouts are being used within the space.
 Client's identify possible visible markers like that seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:calibration-marker"
plural "false"
caps "false"
noprefix "false"

\end_inset

 using thresholding.
 Following this identification, the location of the marker can be found
 within the sensors coordinate system.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andy/uni/dissertation/media/calibration.png
	lyxscale 30
	width 15col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example marker used within the LiveScan3D calibration process
\begin_inset CommandInset label
LatexCommand label
name "fig:calibration-marker"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This information can be used to transform points from the cameras coordinate
 system to the marker's frame of reference.
 As the relative locations of different markers are defined at the server,
 a world coordinate system can be defined as the centre of these markers.
 Typically 4 different markers are placed on the faces around the vertical
 axis of a cuboid allowing views in 360.
\end_layout

\begin_layout Standard
This world coordinate space has shifted the origin from being the position
 of any single 
\noun on
Kinect
\noun default
 sensor to being a point in the centre of the calibration markers that each
 camera now orbits.
 As part of this calibration process, the server distributes transformations
 to each client defining where they sit within this world coordinate space.
 Clients can now transform acquired renders from their native frame of reference
 to the world coordinate system allowing each point cloud to be merged coherentl
y.
\end_layout

\begin_layout Standard
The refinement process is completed server-side by requesting a single frame
 from each connected client and using Iterative Closest Points
\begin_inset CommandInset citation
LatexCommand cite
key "ICP"
literal "false"

\end_inset

 (ICP) to improve the inter-camera relationships.
\end_layout

\begin_layout Standard
The 
\noun on
OpenGL
\noun default
 display space has it's origin within the centre of the visible box, this
 means that for single sensor setups this is also the location of the camera.
\end_layout

\begin_layout Subsection
Buffers and a non-blocking Network
\begin_inset CommandInset label
LatexCommand label
name "subsec:Buffers"

\end_inset


\end_layout

\begin_layout Standard
One of the weaknesses of the native codebase laid in its network behaviour.
 Requests for live frames made by the server to each connected client were
 done synchronously, pausing the operation of the thread responsible for
 passing these frames to the OpenGL display and UEs in order to wait for
 delivery from all clients.
\end_layout

\begin_layout Standard
While this would not be expected to present impaired performance in a controlled
 lab environment, it could prove detrimental when connected over the public
 internet where adverse network conditions and bandwidth bottlenecks are
 more likely.
\end_layout

\begin_layout Standard
In practice, this means that the experience both at the server and any connected
 UEs was determined by the instantaneous network conditions.
\end_layout

\begin_layout Standard
With these public internet applications and the expected network conditions
 in mind, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-buffers"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-buffers"
literal "false"

\end_inset

 provided improvements on this behaviour by including a system of buffers
 throughout the transmission pipeline in order to unblock the network operations.
\end_layout

\begin_layout Standard
Each client queues frames ready for transmission in a buffer while the server
 maintains a buffer at each socket for reception.
 The 
\noun on
KinectServer
\noun default
 collates frames from each socket's Rx buffer and moves them into a live
 buffer for display.
 During display frames are also moved into a Tx buffer ready for transmission
 onto any connected user experiences.
\end_layout

\begin_layout Standard
This system decouples each aspect of the network behaviour, smoothing the
 effect of temporary network conditions.
\end_layout

\begin_layout Standard
Additionally, Selinis' work included the extension of connections between
 both client-server and server-UE to allow the use of multiple concurrent
 TCP connections, effectively increasing the available bandwidth.
 NTP clients were also included at both the client and server allowing the
 timestamps captured at each to be comparable.
 This comparability is assumed when introducing source synchronisation in
 section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Source-Synchronisation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection

\noun on
LiveScan
\noun default
 XR Applications
\begin_inset CommandInset label
LatexCommand label
name "subsec:LiveScan-XR-Applications"

\end_inset


\end_layout

\begin_layout Standard
Developed at the start of 2017, the 
\noun on
LiveScan Hololens
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-hololens"
literal "false"

\end_inset

 project provides an example of XR applications for the 
\noun on
LiveScan
\noun default
 suite.
 
\end_layout

\begin_layout Standard
Using the 
\noun on
Unity
\noun default
 game engine, the application allows the reception and reconstruction of
 
\noun on
LiveScan 
\noun default
point clouds in a head-mounted AR context.
\end_layout

\begin_layout Standard
Utilising a key strength of 
\noun on
Unity
\noun default
's native cross-platform capabilities 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "livescan3d-android"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-android"
literal "false"

\end_inset

 extended the 
\noun on
Hololens
\noun default
 application to build a handheld AR experience targeting the 
\noun on
Android
\noun default
 mobile operating system.
\end_layout

\begin_layout Standard
This was achieved through the static linking of 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 library
\begin_inset CommandInset citation
LatexCommand cite
key "arcore-unity"
literal "false"

\end_inset

 for 
\noun on
Unity
\noun default
 and included buffers in line with the developments discussed in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Buffers"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\noun on
LeanTouch
\noun default

\begin_inset CommandInset citation
LatexCommand cite
key "lean-touch"
literal "false"

\end_inset

 
\noun on
Unity
\noun default
 package was used to allow the live manipulation of displayed holograms,
 abstracting away much of the otherwise required boilerplate input processing
 and management.
\end_layout

\begin_layout Standard
Hologram point clouds are rendered in a similar fashion to the server's
 
\noun on
OpenGL
\noun default
 window in that each vertex of the cloud is rendered individually creating
 the impression of a contiguous mesh as opposed to forming a coherent surface.
 The 
\noun on
PointCloudElem
\noun default
 game object with a 
\lang british
customisable
\lang english
 colour acts as the primitive vertex of this cloud.
\end_layout

\begin_layout Standard
The most relevant scripts for the developments are as follows,
\end_layout

\begin_layout Description
PointCloudReceiver Server-like object responsible for managing connections
 and sockets.
 Responsible for passing received point clouds from the 
\noun on
RxBuffer
\noun default
 to the 
\noun on
PointCloudRenderer
\noun default
 for presentation.
\end_layout

\begin_layout Description
PointCloudRenderer Display manager responsible for managing the live 
\noun on
PointCloudElem
\noun default
s representing a hologram.
 Takes de-buffered frames from the 
\noun on
PointCloudReceiver
\noun default
 and increases or decreases the population of 
\noun on
PointCloudElem
\noun default
s as required before using the contained 
\noun on
ElemRenderer
\noun default
 to update the colour and position of each.
\end_layout

\begin_layout Description
ElemRenderer A component of the 
\noun on
PointCloudElem
\noun default
 responsible for updating the presentation of each vertices' mesh.
\end_layout

\begin_layout Description
PointCloudSocket Traditional socket object for managing network connections.
 Buffers lists of vertices and RGB data from raw bytes received from the
 network.
\end_layout

\begin_layout Subsection
Evaluation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
Here an evaluation of the 
\noun on
LiveScan
\noun default
 suite is presented.
 Its strengths within the space of 3D capture and transmission are identified
 while it's limitations are also highlighted.
\end_layout

\begin_layout Standard
The main strength of the 
\noun on
LiveScan
\noun default
 suite lies in its display agnostic architecture.
 While some of the methods previously reviewed present domain-specific implement
ations of holoportation, such as 
\begin_inset CommandInset citation
LatexCommand cite
key "group-to-group-telepresence"
literal "false"

\end_inset

, 
\noun on
LiveScan
\noun default
 presents a generic way to capture and transmit 3D renders over an IP network
 in real-time.
 The transfer server then provides an interface for different display methods,
 theoretically most forms of XR, to connect and receive these holograms.
\end_layout

\begin_layout Standard
In concept, this design pattern makes it a good candidate for use in a hologram
 streaming service whether for personal communications similar to
\noun on
 Skype
\noun default
 or public broadcast akin to 
\noun on
Twitch
\noun default
 or 
\noun on
Instagram
\noun default
's live streaming functionality.
\end_layout

\begin_layout Standard
Another advantage of the suite lies in the required computation power with
 both client and server able to run on a single fairly powerful computer.
 While systems such as 
\noun on
Microsoft's Mixed Media Studios
\noun default
 present excellent quality mesh-based renders with extensive post-processing,
 there is no expectation for this system to run locally on consumer-grade
 hardware in real-time.
 
\noun on
LiveScan3D
\noun default
 extends access to such acquisition and reconstruction technology in much
 the same way that the 
\noun on
Kinect
\noun default
 did for 3D video capture.
\end_layout

\begin_layout Standard
A limitation of the suite could be identified in its network protocol use,
 TCP connections are used throughout the communication pipeline from 
\noun on
Kinect
\noun default
 sensor to UE.
 While this connection-oriented protocol provides advantages through flow
 control, packet ordering and error handling, these also include overhead
 which limits the speed of transmission.
 For these reasons UDP is typically better suited for media streaming, especiall
y when in real-time.
 Investigations could be made into the suitability for its use in the 
\noun on
LiveScan3D
\noun default
 suite.
\end_layout

\begin_layout Standard
The addition of buffers allows the presentation layer to request frames
 at a constant rate, decoupled from a level of volatility in the network
 layer.
 In doing so, however, the total perceived delay between capture and display
 can be increased as the frames spend time in each of the consecutive buffers.
 In an effort to identify methods for controlling this, investigations are
 made into the effect of intentionally reducing the transmitted frame rate
 in order to reduce this overall delay.
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
comparison between multi-view and multi-source
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Multi-Source Developments
\begin_inset CommandInset label
LatexCommand label
name "sec:Multi-Source-Developments"

\end_inset


\end_layout

\begin_layout Standard
The developments made throughout this project to facilitate multi-source
 experiences were focused on four aspects of the suite,
\end_layout

\begin_layout Itemize
The native display element of the server was updated in order to allow the
 concurrent display of multiple sources.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This included a system of geometric transformations to facilitate the coherent
 arrangement of holograms in the 3D rendered space and a control scheme
 with which to manipulate them.
\end_layout

\end_deeper
\begin_layout Itemize
The network communications throughout the suite were extended to include
 source IDs.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Using this ID, both the server and display elements are able to differentiate
 frames for different sources during processing and presentation.
\end_layout

\end_deeper
\begin_layout Itemize
Methods to temporally synchronise sources were investigated.
\end_layout

\begin_layout Itemize
The mobile AR application was extended in order to display multiple holograms
 simultaneously.
\end_layout

\begin_layout Itemize
Additional features facilitating the paradigm shift away from a single stream
 to a multi-stream scenario.
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Stale 
\emph default
sources for which no network traffic is received within a timeout are identified
 and removed at the server.
\end_layout

\begin_layout Itemize
The global 
\noun on
KinectSetting
\noun default
s object describing parameters including the positions of calibration markers
 was made a source-level object in order to allow multi-view configurations.
\end_layout

\end_deeper
\begin_layout Subsection
Server Display
\end_layout

\begin_layout Standard
This section presents the extensions made to the presentation layer of the
 server application.
 
\end_layout

\begin_layout Standard
The semantics for the implemented geometric transformations are explored
 through the use of the 
\noun on
Transformer
\noun default
 and 
\noun on
DisplayFrameTransformer
\noun default
 classes.
 The final implementation of this geometry within the code is presented.
 The new control scheme is presented, it's weaknesses are considered.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
In designing the extension to the server's OpenGL display window, geometric
 transformations were used in order to arrange holograms in the virtual
 space through rotation and translation.
 This defines each hologram's position within the viewfinder's coordinate
 space, by default arranging them in a circle around the origin.
\end_layout

\begin_layout Standard
In order to maintain a strength of the suite in it's agnostic behaviour
 to different display methods, it was made critical to the design of the
 OpenGL extension that spatial processing would be specific to this display
 only.
 Mobile AR applications will arrange and interact with holograms differently,
 as will head-mounted AR or VR applications.
 In maintaining this separation of network and presentation layer each display
 can operate independently and no assumptions are made as to how a display
 method should handle each hologram.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
During initial testing frames received from a live sensor were intercepted
 and serialized to XML in local storage.
 These frames were then loaded into memory as the server was started and
 merged with those received live before display.
\end_layout

\begin_layout Standard
The composite frame can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-composite-frame"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/pretransform.jpg
	lyxscale 10
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial multi-source composite testing frame
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-composite-frame"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The objects can be seen to be occupying the same space due to their similar
 positions in the frame during capture.
 This is not a sufficient solution for displaying separate sources and so
 geometric transformations like those described above were employed to separate
 the two.
 The change in software structure at this stage can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-testing-layout"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 A rotation of 180 in the vertical (
\begin_inset Formula $y$
\end_inset

) axis pivoted the frames such that they faced those being received live,
 the results can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:180-degree-rotation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/local-testing.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial testing process transforming frames loaded from local storage, 
\noun on
KinectSocket
\noun default
 omitted for clarity
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-testing-layout"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/180flip.jpg
	lyxscale 10
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Composite testing frame following 180 rotation of recorded source in the
 
\begin_inset Formula $y$
\end_inset

-axis
\begin_inset CommandInset label
LatexCommand label
name "fig:180-degree-rotation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to follow this design, the transformations were moved to instead
 occur within the 
\noun on
OpenGL
\noun default
 window class.
 To allow this the shared variables between the 
\noun on
MainWindow
\noun default
 and 
\noun on
OpenGL
\noun default
 were changed.
 A 
\noun on
Frame
\noun default
 object was defined to wrap an individual point cloud with a source ID to
 allow differentiation, the definition can be seen in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The object holds fields for each of the lists previously shared between
 the two objects including a list of vertices (co-ordinates) and the RGB
 values for each as well as the camera poses and bodies.
\end_layout

\begin_layout Standard
The original 
\noun on
LiveScan3D
\noun default
 cleared each of these variables before retrieving a new frame, when moving
 to a multi-source architecture the ability to individually update source
 point clouds was prioritised.
 This would avoid blocking the entire display when unable to receive frames
 from a specific client, other clients would still be able to have frames
 updated promptly.
\end_layout

\begin_layout Standard
To accomplish this a dictionary was used as the shared variable with each
 client's frame referenced by its source ID.
 In doing so only one frame per client is kept and each new frame overrides
 the last.
 During rendering the dictionary is iterated through and each point cloud
 combined.
 During combination, a client-specific transformation is retrieved from
 an instance of the 
\noun on
DisplayFrameTransformer
\noun default
 class.
 This object is a member of the 
\noun on
OpenGL
\noun default
 window and is responsible for defining the orientation and position of
 each point cloud.
\end_layout

\begin_layout Subsubsection
Geometric Transformations
\end_layout

\begin_layout Standard
Within the 
\noun on
LiveScan3D
\noun default
 server source code are utility structures and classes which were extended
 in order to develop a wider geometric manipulation system.
 Structures defining Cartesian coordinates in both 2D and 3D spaces called
 
\noun on
Point2f
\noun default
 and 
\noun on
Point3f
\noun default
 respectively are used in drawing skeletons as captured by the 
\noun on
Kinect
\noun default
 camera, there is also a class defining an affine transformation.
\end_layout

\begin_layout Standard
Affine transformations are a family of geometric transformations that preserve
 parallel lines within geometric spaces.
 Some examples of affine transformations include scaling, reflection, rotation,
 translation and shearing.
\end_layout

\begin_layout Standard
The class definition is made up of a three-by-three transformation matrix
 and a single 3D vector for translation, within the original codebase it
 is used for both camera poses and world transformations.
 
\end_layout

\begin_layout Standard
A camera pose is the affine transformation defining the position and orientation
 of the 
\noun on
Kinect
\noun default
 camera when drawn in the 
\noun on
OpenGL
\noun default
 space as a green cross.
 The world transformations are used as part of the calibration process when
 using multi-view configurations.
\end_layout

\begin_layout Standard
When considering how each source's render would be arranged in the space,
 the use of this class definition was extended.
 As the use of affine transformations is mostly limited to use as a data
 structure within the base source code, some utility classes and functions
 were required in order to fully maximise their effectiveness.
\end_layout

\begin_layout Paragraph

\noun on
Transformer
\end_layout

\begin_layout Standard
The motivation in writing the 
\noun on
Transformer
\noun default
 was to create a generic framework of geometric transformations that could
 be utilised by the 
\noun on
OpenGL
\noun default
 display to arrange separate point clouds.
 At a high level, this is done by implementing matrix arithmetic functions
 in the context of their use for applying linear transformations to Cartesian
 coordinates.
 
\end_layout

\begin_layout Standard
The 
\noun on
Transformer
\noun default
 class has static methods to apply 
\noun on
AffineTransform
\noun default
s to both 
\noun on
Point3f
\noun default
 structures and lists of raw vertices as received from 
\noun on
LiveScan
\noun default
 clients.
\end_layout

\begin_layout Standard
Additionally, utility functions to bidirectionally cast between 
\noun on
Point3f
\noun default
 data structures and the lists of received vertices were written.
\end_layout

\begin_layout Standard
Finally, static methods generate common rotation transformations about each
 axis given an arbitrary angle employing Euler angles.
 This provided a foundation on which to define how the 
\noun on
OpenGL
\noun default
 space would arrange separate sources within it's combined co-ordinate space.
\end_layout

\begin_layout Subsubsection

\noun on
DisplayFrameTransformer
\end_layout

\begin_layout Standard
The 
\noun on
DisplayFrameTransformer
\noun default
 is responsible for maintaining the geometric state of each source, generating
 transformations for each displayed within the 
\noun on
OpenGL
\noun default
 window at each frame update.
 A UML diagram for the class can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:UML-displayframetransformer"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/DisplayFrameTransformer.png
	lyxscale 20
	width 75col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
UML diagram for 
\noun on
DisplayFrameTransformer
\noun default

\begin_inset CommandInset label
LatexCommand label
name "fig:UML-displayframetransformer"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Each source is assigned a default transformation which can be overridden
 using keyboard controls.
\end_layout

\begin_layout Standard
Sources are initially arranged in a circle around the origin in the centre
 of the space.
 This is done by retrieving a transformation from the 
\noun on
Transformer
\noun default
 for a rotation in the 
\begin_inset Formula $y$
\end_inset

-axis for each source, 
\begin_inset Formula $n$
\end_inset

.
 Each angle of rotation, 
\begin_inset Formula $\alpha$
\end_inset

, is calculated using the below, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha\left(n\right)=\frac{n}{\sum sources}\cdotp360\textdegree
\]

\end_inset


\end_layout

\begin_layout Standard
Similar to the shared variables between the 
\noun on
MainWindow
\noun default
 and 
\noun on
OpenGL
\noun default
 window, source transformations are stored within a dictionary indexed by
 source ID.
\end_layout

\begin_layout Standard
The 
\noun on
DisplayFrameTransformer
\noun default
 also has methods to override these initial transforms with the RotateSource()
 and TranslateSource() methods.
 When these methods are called for the first time on a source, an object
 defining the position and rotation is populated using the default rotation.
 From here the presence of a source override results in applied transforms
 being defined by these values as opposed to the default orientation.
\end_layout

\begin_layout Standard
This leaves the current architecture of the server application as described
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:current-state-diagram"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/december-state.png
	lyxscale 30
	width 65col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Current state of 
\noun on
LiveScan
\noun default
 server structure with 
\noun on
OpenGL
\noun default
 window-based transformer (
\noun on
KinectSocket
\noun default
 omitted for clarity)
\begin_inset CommandInset label
LatexCommand label
name "fig:current-state-diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Control Scheme
\end_layout

\begin_layout Standard
The movement of objects within the 
\noun on
OpenGL
\noun default
 space is implemented through keyboard controls.
 While using the mouse would allow fine-grained and intuitive control, the
 number of axes for motion and rotation available to objects makes defining
 specific keys for each more flexible.
 This additionally removes the need to redefine or overload the camera controls.
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

I
\begin_inset Quotes erd
\end_inset

 key is used to cycle through displayed sources, the currently selected
 source is the subject of each of the movement actions.
 Sources are moved across the horizontal plane (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $x$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $z$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
) of the display space using a WASD-esque layout of the UHJK keys.
 Objects can be rotated about the vertical (
\begin_inset Formula $y$
\end_inset

) axis using the B and N keys.
 Finally, the placement of an object can be reset to default using the R
 key, the addition of the shift modifier resets all clients.
\end_layout

\begin_layout Standard
Worth noting is that this represents arbitrary placement of sources in only
 two axes of position and one of rotation.
 This was a conscious choice as these are the most common and intuitive
 axes with which sources will need to be manipulated.
 The ability to allow movement in all axes would require only binding these
 actions to keys.
\end_layout

\begin_layout Subsection
Network Source IDs
\end_layout

\begin_layout Standard
The following section explores how the network communications throughout
 the 
\noun on
LiveScan
\noun default
 suite were extended in order to identify the source each frame corresponds
 to.
\end_layout

\begin_layout Standard
The original and updated packet structures are explored.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
Within the 
\noun on
LiveScan
\noun default
 suite, two network communications required updating, those between the
 client and server and those between the server and user experiences.
\end_layout

\begin_layout Standard
In the native codebase the headers for all packets were the same, the structure
 can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Existing-packet-header"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderSTD.png
	lyxscale 50
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Native packet header structure for all 
\noun on
LiveScan
\noun default
 TCP packets
\begin_inset CommandInset label
LatexCommand label
name "fig:Existing-packet-header"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The message type field indicates what action is being conducted with examples
 including a request for a frame from the server to the client or the delivery
 of settings.
 The message types can be seen in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Network-Message-Types"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 While there are many forms of message sent between client and server including
 settings delivery and calibration requests, messages from server to UE
 currently only correspond to frame delivery.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
An integer source ID was created to be included in communications throughout
 the suite.
 For conciseness, this was represented by a single byte during transmission
 resulting in a range of possible source IDs between 0 and 127.
 Considering the use case of a source representing a scene and the bandwidth
 required for each, 128 possible concurrent sources was deemed sufficient.
\end_layout

\begin_layout Standard
When and where the source ID was included within each packet was decided
 by the semantics behind the different kinds of message.
 For server to UE transmissions the source ID was included within the header
 of the packet as can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:new-ue-packet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderUE.png
	lyxscale 50
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Updated packet structure for server to UE communications, blue highlight
 indicates new field
\begin_inset CommandInset label
LatexCommand label
name "fig:new-ue-packet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
With regards to client to server communications it was noted that not all
 message types have a natural association with the source itself.
 For example when the server requests a client to begin calibration there
 is no requirement for the source ID to be present.
 As such the source ID is instead included only during frame delivery from
 client to server at the start of the payload.
 This layout can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:new-client-server-packet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/HeaderClientServer.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Updated packet structure for client to server frame delivery, blue highlight
 indicates new field
\begin_inset CommandInset label
LatexCommand label
name "fig:new-client-server-packet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The client has had a text box added to the UI in order to allow an integer
 source ID to be added.
 This value is clamped between 0 and 127 to fit within the allowed range
 and is then attached to each frame delivery as described above.
\end_layout

\begin_layout Subsection
Source Synchronisation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Source-Synchronisation"

\end_inset


\end_layout

\begin_layout Standard
As a service intending to allow real-time streaming from multiple locations,
 methods to synchronise streams during display constitute important aspects
 of the experience.
 With reference back to traditional conference-calling, voices being out
 of sync poses a hurdle to seamless communication.
 When considering these instead relating to holograms and the higher level
 of intended immersion, this could hinder the resulting experience of telepresen
ce.
\end_layout

\begin_layout Standard
At a high level, sources can be considered synchronised if frames captured
 at the same time are displayed concurrently whether from the perspective
 of the server or a connected user experience.
 There are two main challenges to achieving this aim.
 The following assumes that timestamps captured at both the client and server
 are synchronised and therefore comparable using the included NTP functionality
\begin_inset CommandInset citation
LatexCommand cite
key "livescan3d-buffers"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Although capturing frames at the same rate (30Hz), it is highly unlikely
 that clients will capture frames at the same times.
 A frequency of 30Hz corresponds to an interval of 33ms between frame captures
 allowing a maximum offset of 17ms (
\begin_inset Formula $\nicefrac{33}{2}$
\end_inset

) between clients.
 While the latency incurred as a result may not be noticeable and will likely
 not represent a large proportion of the overall latency, it does imply
 that frames cannot be matched by checking timestamps for equality and instead
 a latency interval must be used.
\end_layout

\begin_layout Standard
Secondly, during transmission a variable amount of latency will be introduced
 as a result of the network conditions and distance from client to server
 and from server to UE.
\end_layout

\begin_layout Standard
Methods to synchronise clients at the server could be classified as either
 
\emph on
active
\emph default
 or 
\emph on
passive
\emph default
.
 
\end_layout

\begin_layout Subsubsection
Passive Synchronisation
\end_layout

\begin_layout Standard

\emph on
Passive
\emph default
 synchronisation describes methods aiming to indirectly reduce the difference
 in latency as a result of controlling other service conditions.
 Applying a latency requirement by controlling frame rate as investigated
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is a method of passive synchronisation.
 By prescribing a latency requirement to all sources, each should tend to
 this value once received at the server.
 
\end_layout

\begin_layout Standard
Passive methods would likely not be effective at tightly synchronising sources,
 the FPS/latency balancing method described previously aims only to bring
 the otherwise unbounded latency differences within a tolerable interval
 and controls for a level of network volatility.
\end_layout

\begin_layout Subsubsection
Active Synchronisation
\end_layout

\begin_layout Standard

\emph on
Active
\emph default
 synchronisation includes efforts to tightly match frames during processing
 at the server in order to minimise the difference in latency since capture.
 In practice, this could be achieved at the point where frames are moved
 from the various receiving buffers to the live buffer
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
implementation?
\end_layout

\end_inset

.
 Positioned within the critical processing pipeline, active methods have
 access to only a tight interval of frames and must operate quickly making
 them more sensitive to wide latency differences in sources.
\end_layout

\begin_layout Standard
In practice, with 
\emph on
passive
\emph default
 sync functioning as coarse control and 
\emph on
active
\emph default
 sync acting as fine control, a combination of the two will likely provide
 the best results.
 As described, passive methods will be most effective at bringing the unbounded
 latencies to within an interval of tolerance allowing active measures to
 finish matching frames for a tighter synchronisation.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
As a result of the lack of access to multiple 
\noun on
Kinect
\noun default
 sensors, the implemented multi-source capabilities were unable to be tested.
 Without a live multi-source environment, active synchronisation methods
 also remain presented here in theory.
\end_layout

\begin_layout Standard
In order to present developments in source synchronisation, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 functions as an investigation into the efficacy of one form of passive
 synchronisation by implementing a latency requirement which each client
 works to meet.
\end_layout

\begin_layout Subsection
Mobile AR
\begin_inset CommandInset label
LatexCommand label
name "subsec:Mobile-AR"

\end_inset


\end_layout

\begin_layout Standard
Here the multi-source updates made to the mobile AR application are presented.
 In order to complete this update two objectives must be achieved,
\end_layout

\begin_layout Itemize
The network and rendering behaviour must become source ID-aware, able to
 differentiate and render separate scenes
\end_layout

\begin_layout Itemize
The touch input management must be restructured to support the individual
 manipulation of separate holograms
\end_layout

\begin_layout Standard
The architecture of the mobile AR application can be divided into two areas
 of concern.
 The first is related to establishing the AR environment within the 
\noun on
Unity
\noun default
 scene including plane discovery, motion tracking and presenting the camera
 feed as the background.
 This is provided by the 
\noun on
Google ARCore
\noun default
 library and requires only configuring provided objects, no additional code
 is required.
 The other aspect is the 
\noun on
LiveScan
\noun default
 specific objects responsible for receiving and constructing holograms within
 the AR scene.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/UnityScriptsBefore.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Existing architecture of mobile AR components
\begin_inset CommandInset label
LatexCommand label
name "fig:Existing-scripts"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This 
\noun on
LiveScan
\noun default
 specific code can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Existing-scripts"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:LiveScan-XR-Applications"
plural "false"
caps "false"
noprefix "false"

\end_inset

 the 
\noun on
PointCloudReceiver
\noun default
 acts as the network layer passing frames from the receiving buffer to the
 
\noun on
PointCloudRenderer
\noun default
 for reconstruction in the form of a collection of 
\noun on
PointCloudElems
\noun default
.
 This collection of 
\noun on
PointCloudElem
\noun default
s is scaled in population to match the size of the hologram and then each
 is re-coloured and positioned on each frame update.
\end_layout

\begin_layout Subsubsection
Implementation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Mobile-AR-Multisource-Implementation"

\end_inset


\end_layout

\begin_layout Standard
The nature of developing in 
\noun on
Unity
\noun default
 allowed the restructuring of this architecture into one capable of supporting
 multi-source experiences with minimal code additions.
 A source prefab
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
does prefab need defining?
\end_layout

\end_inset

 was created with the intention of encapsulating the necessary components
 required to represent a whole source including its presentation and touch
 input management.
 The 
\noun on
PointCloudRenderer
\noun default
 was redefined as this presentation manager for a single source allowing
 it to own the 
\noun on
PointCloudElem
\noun default
s making up a hologram.
\end_layout

\begin_layout Standard
On each frame update, the 
\noun on
PointCloudReceiver
\noun default
 retrieves the required source from a dictionary indexed by source ID or
 instantiates one if it is not available
\noun on
.

\noun default
 The render function of the source's contained renderer can then be called.
 This updated architecture can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Scripts-after"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/UnityScriptsAfter.png
	lyxscale 30
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Restructured architecture of mobile AR components
\begin_inset CommandInset label
LatexCommand label
name "fig:Scripts-after"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before losing access to the lab environment the ability to individually
 manipulate holograms was not implemented leaving this to be completed beyond
 the end of the project.
 Looking to the 
\noun on
LeanTouch
\noun default
 documentation, the required restructuring of components is specifically
 outlined suggesting that with access to a compatible mobile device the
 application could theoretically be completed without intense development.
\end_layout

\begin_layout Subsection
Stale Source Culling
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stale-Source-Culling"

\end_inset


\end_layout

\begin_layout Standard
Transitioning the 
\noun on
LiveScan
\noun default
 suite to a multi-source architecture implies a significant difference in
 certain behaviours of the server.
 When encountering adverse network conditions in a single source scenario,
 the desired action could be to wait until transmissions from the client(s)
 can resume.
 With only a single stream, the alternative would be to halt the experience.
\end_layout

\begin_layout Standard
However in a multi-source scenario, a decision could be made to instead
 remove sources that cannot maintain a reasonable transmission rate so as
 not to infringe on the experience of other sources, no longer do the conditions
 of any single stream define the conditions of the whole.
\end_layout

\begin_layout Standard
A similarity could be drawn between watching a traditional broadcast sports
 game compared to 
\emph on
simulcast
\emph default
 broadcasts such as those seen on the 
\noun on
NFL's RedZone
\noun default
 where multiple games can be watched simultaneously, dividing the screen.
 Were one of the games to experience transmission issues, it could be considered
 beneficial to the experience to remove the game from display and wait for
 the conditions to improve, especially in a commercial context.
\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
A source could be considered 
\emph on
stale
\emph default
 when a frame has not been received for it within a certain time threshold,
 the default was decided as 5 seconds.
 Beyond this, the source should be considered, if at least temporarily,
 disconnected.
\end_layout

\begin_layout Standard
This was achieved using a separate thread that periodically iterates through
 the last frame of each source and compares an associated timestamp to the
 current time.
\end_layout

\begin_layout Standard
While this thread could be included in many places throughout the server
 application, it was considered important to facilitate applicability to
 any cross-platform developments.
\end_layout

\begin_layout Standard
As such the object-oriented programming concept of encapsulation was employed
 to create an object responsible for containing live frames and checking
 for 
\emph on
stale
\emph default
 sources.
\end_layout

\begin_layout Subsubsection

\noun on
SourceCollection
\end_layout

\begin_layout Standard
The 
\noun on
SourceCollection
\noun default
 object wraps around the dictionary object used to store live frames indexed
 by source ID.
 The previously described thread is implemented along with integer attributes
 defining the interval at which frames are checked and the threshold for
 which sources are designated 
\emph on
stale
\emph default
.
 The UML diagram for this object can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:source-collection"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/SourceCollection.png
	lyxscale 20
	width 60col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
UML diagram for the 
\noun on
SourceCollection
\noun default

\begin_inset CommandInset label
LatexCommand label
name "fig:source-collection"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to lend the design to a cross-platform server, events are used
 to signal a source being connected or disconnected.
 A connected source is identified when a frame is added for which there
 is no key found in the sources dictionary.
 The disconnected source event fires during the RemoveSource() method which
 is called manually or by the cleaner thread.
\end_layout

\begin_layout Standard
An example of the use of these events is found in the 
\noun on
DisplayFrameTransfomer
\noun default
 where a check is made to ensure that a source's position override is cleared
 if one exists when that source is disconnected.
\end_layout

\begin_layout Subsection
Multi-Source Settings
\end_layout

\begin_layout Standard
The 
\noun on
KinectSettings
\noun default
 class is responsible for maintaining configuration parameters of the experience
 including calibration for multi-view scenarios and flags for whether bodies
 should be isolated from the scene and whether skeletons should be shown
 in the server display.
 Some of these, including the positions of calibration markers must be known
 by clients and as such the 
\noun on
KinectServer
\noun default
 is capable of delivering the current settings to each client.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
settings window figure?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Design Considerations
\end_layout

\begin_layout Standard
When considering the transition from a single stream scenario to the multi-sourc
e product, it can be seen that the settings should no longer be global.
 Taking the calibration settings as an example, the positions and ID's of
 each marker refers to those visible within a scene.
 These will have no meaning if made central parameters, they are closely
 tied to the scene and therefore the source.
 Each source should have its own independent set of settings and the server
 should be able to deliver the settings to the corresponding clients.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
The 
\noun on
SourceCollection
\noun default
 presented a natural location to store each source's settings object alongside
 its last frame, again aiming to employ the object-oriented ethos of encapsulati
on.
 This allowed the settings to be governed by the existing logic including
 
\emph on
stale source
\emph default
 identification.
\end_layout

\begin_layout Standard
The global settings object was not removed but instead had it's function
 changed.
 The object was altered to define the default settings on which each newly
 connected source's are based on.
\end_layout

\begin_layout Subsection
Evaluation and Discussion
\end_layout

\begin_layout Standard
As described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Mobile-AR-Multisource-Implementation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, holograms cannot currently be individually manipulated by the mobile AR
 application.
 Although theoretically a simple restructuring, as-is the application cannot
 be considered a complete multi-source presentation layer.
\end_layout

\begin_layout Standard
Currently there is no way to negotiate source IDs with a newly connected
 source.
 It is the responsibility of each client of a source to select the same
 ID.
 It is also feasible for a new source to select an occupied ID without challenge
 resulting in the server merging both sources together.
 This would likely result in the two holograms flickering in the same space
 and scatter calculated statistics.
 Although attempts could be made to identify clients by IP address, this
 would prove ineffective in many situations including IP addresses behind
 carrier-grade NAT, those using IPv6 addressing or co-located sources.
 Co-located sources could occur at a hypothetical studio utilising the 
\noun on
LiveScan3D
\noun default
 suite.
 One alternative could be for the client to use an alphanumeric identifier
 which would be mapped to the integer source ID during connection.
 This handshake would then theoretically allow easier implementation of
 further validation including authentication methods like passwords.
\end_layout

\begin_layout Standard
The use of a source ID transmitted alongside each frame provides flexibility
 in that processing can be decoupled from the network infrastructure itself.
 Theoretically, it would also allow clients to function as multiple sources
 simultaneously, allowing the attachment of different source IDs to different
 frames
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
While no applications for such a configuration are posed here, this could
 prove interesting with further investigation.
\end_layout

\end_inset

.
 However, the constant transmission of a source ID also extends the possibility
 for error.
 During debugging, what was assumed to be a malformed packet could cause
 a frame to be entered for a different source as the source ID was corrupted.
 While a single frame error doesn't necessarily constitute a catastrophic
 error, it could prove detrimental to the experience.
 
\end_layout

\begin_layout Standard
An alternative would be to associate a socket with a source ID, negotiated
 during connection.
 Frames received over this socket could then be tagged with this ID for
 further propagation.
 While this would not inherently fix the previously mentioned malformed
 packet, it would constrain it to the source for which it was received as
 opposed to allowing the source ID to also be subject to corruption, possible
 affecting other sources.
\end_layout

\begin_layout Standard
The server display's control scheme could be more intuitive as the directions
 of movement are in relation to the fixed axes of the display space instead
 of the view of the camera.
 In practice, this means that when translating objects the pose of the camera
 with respect to the space must first be considered in order to identify
 which direction the object will be moved.
\end_layout

\begin_layout Standard
This is less intuitive than could be expected in other areas where such
 control schemes are used such as video games or modelling software.
 In such implementations when moving objects the directions are typically
 taken from the camera's frame of reference.
 
\end_layout

\begin_layout Standard
This could be implemented by updating the 
\noun on
Transformer
\noun default
 class to include a matrix inversion function.
 Applying Euler angles through the already implemented rotation retrieval
 functions, desired translations in the camera's frame of reference could
 then be shifted back to that of the world's for application to a hologram.
\end_layout

\begin_layout Standard
As a whole, the design of the multi-source developments was intended to
 follow the philosophy of the existing suite without radically redesigning
 the application.
\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Standard
Within this section the multi-source developments made throughout the 
\noun on
LiveScan
\noun default
 suite have been presented.
 The suite is now capable of supporting multi-source experiences from client
 to mobile AR application.
 The viewfinder display at the server also provides an interactive space
 to view all connected sources.
 The limitations of the mobile AR application have been described, notably
 the lack of individual touch interaction.
 Two concepts of synchronisation have been introduced, 
\emph on
active
\emph default
 and 
\emph on
passive
\emph default
 sync.
 A method of 
\emph on
passive
\emph default
 sync will be investigated in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frame-Rate-Throttling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\emph on
Stale
\emph default
 sources were defined as sources that have not transmitted any frames within
 a set time interval in order to remove them from display.
 Finally, the global 
\noun on
KinectSettings
\noun default
 object was redefined as a per-source object, the main advantage of which
 being to allow multi-view sources.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Cross-Platform Mobile AR with 
\noun on
ARFoundation
\begin_inset CommandInset label
LatexCommand label
name "sec:Cross-Platform-Mobile-AR"

\end_inset


\end_layout

\begin_layout Standard
This section presents the refactoring of the mobile AR application conducted
 in response to the lack of access to a compatible 
\noun on
Android
\noun default
 device, see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:COVID-19"
plural "false"
caps "false"
noprefix "false"

\end_inset

, COVID-19.
\end_layout

\begin_layout Standard
Although the majority of multi-source functionality had been developed prior
 to the loss of access to the lab environment, the ability to individually
 manipulate holograms through touch controls remained to be implemented.
 The ability to deploy the application to an available iOS device would
 allow this development to resume while also allowing continued debugging
 of the network behaviour.
\end_layout

\begin_layout Standard
Two methods were investigated, deploying as-is using 
\noun on
ARCore
\noun default
 for 
\noun on
Unity
\noun default
's iOS support and migrating to 
\noun on
Unity
\noun default
's 
\noun on
ARFoundation
\noun default
 framework.
\end_layout

\begin_layout Subsection

\noun on
ARCore
\noun default
 for iOS
\end_layout

\begin_layout Standard
As described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Handheld-Augmented-Reality"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the existing mobile application constructs an AR environment using 
\noun on
Google's
\noun default
 
\noun on
ARCore
\noun default
 
\noun on
Unity
\noun default
 package.
 Consulting the documentation for this package, support is provided for
 deploying to both 
\noun on
Android
\noun default
 and iOS and as such this avenue was investigated first.
\end_layout

\begin_layout Standard
Following configuring the build settings as directed, the application failed
 to operate correctly on deployment as permission for the camera was not
 requested.
 As a result, the application could not present the 
\emph on
Window on the World
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "reality-virtuality-continuum"
literal "false"

\end_inset

 effect of rear-camera pass-through natural to handheld AR experiences.
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
expand?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\noun on
Unity ARFoundation
\end_layout

\begin_layout Standard
Following these issues, a different avenue was pursued in 
\noun on
Unity
\noun default
's native 
\noun on
ARFoundation
\noun default
 framework.
 While this would present a significant restructuring of the application
 it would also theoretically expand the functionality allowing continued
 deployment to 
\noun on
Android
\noun default
 while including iOS as an install target
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
With 
\noun on
Hololens
\noun default
 plane tracking pending support in 
\noun on
ARFoundation
\noun default
 at the time of writing, support could theoretically also be extended back
 to 
\noun on
Hololens
\noun default
 in the future with minimal extra development.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Migrating the 
\noun on
ARCore
\noun default
 components constituting the AR environment to 
\noun on
ARFoundation
\noun default
 did not involve an intense redesign as the libraries have similar design
 principles and structure, many components were presented as a direct replacemen
t.
 The three main aspects of the AR environment include the concept of an
 AR session, camera management and the detection of features within the
 world.
\end_layout

\begin_layout Standard
Both libraries use a session component to manage the life cycle of the AR
 experience with a 
\noun on
Unity
\noun default
 camera acting as the virtual representation of the mobile's rear-view camera.
 In both cases, this virtual camera has a pose driver attached responsible
 for tracing the movements of the physical device in the virtual space and
 a component responsible for displaying the view of the rear-facing camera
 as the background of the scene.
 This presents the core of the environment, additionally, both libraries
 include separate components responsible for displaying discovered planes
 within the space, optionally with UI directing the user to aid in this
 discovery.
 A difference in hierarchy was found in environmental light estimation allowing
 holograms to sit more naturally within the rendered space, presented as
 an option of the 
\noun on
CameraManager
\noun default
 component of 
\noun on
ARFoundation
\noun default
 and a separate prefab object in 
\noun on
ARCore
\noun default
.
\end_layout

\begin_layout Subsection
Results and Discussion
\end_layout

\begin_layout Standard
Following the migration to 
\noun on
ARFoundation
\noun default
 the application starts and constructs an AR environment as expected.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:app-ui-planes"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates the UI of the application allowing a server IP to be entered,
 highlighting the native keyboard and rear-camera pass-through.
 Plane discovery can be seen in both images with the desk and floor correctly
 identified.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ios_app/IMG_2585.PNG
	lyxscale 15
	width 25col%

\end_inset

 
\begin_inset Graphics
	filename ../media/ios_app/IMG_2586.PNG
	lyxscale 15
	width 25col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The UI and discovered planes of the mobile AR application running on iOS
\begin_inset CommandInset label
LatexCommand label
name "fig:app-ui-planes"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Exercising another strength of the 
\noun on
Unity
\noun default
 environment, none of the written code scripts required editing with network
 behaviour functioning, to qualitative tests, the same as seen on 
\noun on
Android
\noun default
.
 As mentioned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Mobile-AR"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the AR environment constructed either by 
\noun on
ARCore
\noun default
 or 
\noun on
ARFoundation
\noun default
 functions somewhat independently from the 
\noun on
LiveScan
\noun default
 specific components.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ios-holograms"
plural "false"
caps "false"
noprefix "false"

\end_inset

, however, demonstrates the limitations of the application in this form.
 As it stands the rendering of holograms is non-functional as a result of
 an incompatible shader leaving the presented purple shapes.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ios_app/IMG_2589.PNG
	lyxscale 15
	width 25col%

\end_inset

 
\begin_inset Graphics
	filename ../media/ios_app/IMG_2592.PNG
	lyxscale 15
	width 25col%

\end_inset

 
\begin_inset Graphics
	filename ../media/ios_app/IMG_2595.PNG
	lyxscale 15
	width 25col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Holograms rendered on iOS with the included incompatible shader
\begin_inset CommandInset label
LatexCommand label
name "fig:ios-holograms"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\noun on
GS Billboard
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A billboard here refers to creating a 2D shape typically facing the camera
 plane similar to a billboard sign, they are sometimes used to simplify
 the rendering of complex objects.
\end_layout

\end_inset

 
\noun default
shader included with the application is a simple 
\emph on
geometry shader
\emph default
 used to render the 
\noun on
PointCloudElem
\noun default
s in a pixel-like fashion.
 As described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:LiveScan-XR-Applications"
plural "false"
caps "false"
noprefix "false"

\end_inset

 this mirrors the design philosophy of the server viewfinder.
 While this suits the previous 
\noun on
Android
\noun default
 environment utilising the 
\noun on
OpenGL
\noun default
 ES3 graphics library, when compiling for iOS and hence 
\noun on
ARKit,
\noun default
 the native 
\noun on
Metal
\noun default
 graphics library is used which does not support geometry shaders.
\end_layout

\begin_layout Standard
While there was an opportunity here to find an alternative shader in order
 to complete the migration, with a lack of graphics programming experience
 this proved to be beyond the scope of this project.
\end_layout

\begin_layout Standard
The final product, however, did prove useful both in demonstrating the cross-pla
tform capabilities of the application and in allowing continued network
 debugging of the suite without access to an 
\noun on
Android
\noun default
 device.
\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Standard
Within this section, efforts made to extend compatibility of the mobile
 AR application to iOS devices have been presented.
 The goal of migrating the AR SDK from 
\noun on
Google's ARCore
\noun default
 to 
\noun on
Unity's ARFoundation
\noun default
 was successful and the app would likely function correctly on 
\noun on
Android
\noun default
.
 However, the app does not display holograms on iOS due to the included
 shader being incompatible.
 The app still functions as a useful network debugging tool.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Balancing Frame Rate and Transmission Latency
\begin_inset CommandInset label
LatexCommand label
name "sec:Frame-Rate-Throttling"

\end_inset


\end_layout

\begin_layout Standard
Operating a server in the cloud could incur long latencies between client
 frame capture and display whether at the server or user experience.
 As discussed in the evaluations for the 
\noun on
LiveScan
\noun default
 suite (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Evaluation"
plural "false"
caps "false"
noprefix "false"

\end_inset

), the inclusion of buffers can also add to the total effective delay of
 frames from capture to presentation as they propagate through each, specificall
y the transmission buffer of the client.
\end_layout

\begin_layout Standard
A proposed hypothesis for reducing this delay is to limit the transmitted
 frame rate.
 It follows that this would reduce the population of each buffer and reduce
 the time that each frame spent within each.
\end_layout

\begin_layout Standard
This was implemented based on further work by Selinis introducing the concept
 of a 
\emph on
dynamic step
\emph default
.
 This step is calculated from an FPS and latency requirement and represents
 the percentage of frames to be probabilistically dropped by the client.
 This step is transmitted in the header of each frame request theoretically
 allowing the system to dynamically respond to changes in network conditions.
 This step relied on global statistics calculated about the 
\noun on
KinectServer
\noun default
 including FPS and network bandwidth.
\end_layout

\begin_layout Subsection
Design Considerations
\end_layout

\begin_layout Standard
When considering the final applications of these concepts in the context
 of a multi-source project, the ability to have a step per-source was included
 in order to take into account the different network conditions between
 the server and each source's client(s).
 A single set of global FPS and delay requirements would still be provided
 and a step for each source would be calculated and distributed.
\end_layout

\begin_layout Standard
In order to implement these concepts and evaluate the posed hypothesis the
 following was required,
\end_layout

\begin_layout Itemize
Extend the provided stats calculation function to do so for each source
 as opposed to globally.
\end_layout

\begin_deeper
\begin_layout Itemize
This allows source-specific dynamic steps to be used.
\end_layout

\end_deeper
\begin_layout Itemize
Implement a moving average scheme on successive subsets of latencies from
 client to server.
\end_layout

\begin_layout Itemize
Allow either an FPS or delay requirement to be provided as well as the existing
 combination of the two in order to facilitate the prioritisation of either.
\end_layout

\begin_layout Subsection
Multi-Source Stats
\end_layout

\begin_layout Standard
The existing calculations of live statistics provided the global FPS and
 network bandwidth of the 
\noun on
KinectServer
\noun default
.
 In order to collect these values on a source-by-source basis, a dictionary
 of 
\noun on
SourceStat
\noun default
 objects was used.
 This object collects the FPS, bandwidth and average latencies for each
 source allowing simple retrieval and update when iterating through sockets.
\end_layout

\begin_layout Standard
In implementing the latency calculations both a simple moving average or
 traditional mean and exponential moving average were taken in order to
 investigate the efficacy of both.
 The equation for exponential moving average can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Exponential-moving-average"
plural "false"
caps "false"
noprefix "false"

\end_inset

, this iterative version was well suited to processing each collected latency
 value.
 Initially, the values were found by collecting the latency of each frame
 currently within the server's buffers.
 Theoretically, this would have allowed the calculated average to be controlled
 by the speed with which frames are moved through the buffer.
 However, this was found to be ineffective as the buffers typically stay
 empty.
 Instead, a queue of fixed size was defined in order to retain the most
 recent values of latency received at the server.
 This queue included calculations for both exponential and simple moving
 averages of the contents.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula 
\[
S_{t}\begin{cases}
Y_{1}, & t=1\\
\alpha\cdot Y_{t}+\left(1-\alpha\right)\cdot S_{t-1} & t>1
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $\alpha$
\end_inset

 = Weighting factor
\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $Y_{t}$
\end_inset

= Value at time 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Formula $S_{t}$
\end_inset

= EMA value at time 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Exponential moving average calculation
\begin_inset CommandInset citation
LatexCommand cite
key "ema"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Exponential-moving-average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Step Calculation & Use
\end_layout

\begin_layout Standard
In order to function as a proof-of-concept for future investigations, a
 simple method for calculating the frame drop rate was used.
 The step began at 0% and was incremented or decremented linearly depending
 on the source's latency or FPS with respect to the provided requirements,
 an interval of 5% was used.
 
\end_layout

\begin_layout Standard
In practice, this meant that for each second a source's latency was higher
 than the requirement, for example, the source's frame drop rate would be
 increased by 5%.
 The implementation in code can be seen in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Linear-Frame-Drop"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Each frame, the client generates a random number between 0 and 1 and compares
 this to the step.
 If this random number is greater than the step then this frame is transmitted,
 otherwise it is dropped.
\end_layout

\begin_layout Standard
Future investigations could use a non-linear calculation in order to reflect
 the distance operating values are from the requirements.
 Theoretically, this would allow the step to move rapidly and quickly approach
 the requirements before slowing and settling.
\end_layout

\begin_layout Subsection
Testing Methodology
\end_layout

\begin_layout Standard
The following outlines the methods used to investigate the effect of limiting
 client transmission frame rates in order to control the effective display
 latency between a client and server.
 Similar control methods could be implemented between server and user experience
 device with similar expected results.
 Both test environments are presented and the validity of each is discussed.
\end_layout

\begin_layout Subsubsection
Premise Validation
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/TestDev.png
	lyxscale 30
	width 45col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Client-server relationship in a development environment located on a single
 machine
\begin_inset CommandInset label
LatexCommand label
name "fig:TestDev"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Development was conducted with the client and server located on the same
 machine reducing the latency to just that of processing the frames themselves,
 see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TestDev"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In order to validate the premise of the hypothesis, investigations were
 made on two machines across a domestic local area network.
\end_layout

\begin_layout Standard
When situated on the same machine the bandwidth used by the suite averages
 around 275Mbps for a full scene.
 When transmitting only bodies, these numbers can be more dynamic based
 on the number and size of figures in the frame.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:body-bandwidth-variation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the variation in network bandwidth for a single body moving closer
 to and further away from the camera, it can be seen to vary between 5 and
 125 Mbps.
 These are taken as the ideal or desired bandwidth requirements for both
 configurations.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/BodyBandwidthVariation.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Variation in used bandwidth for a single body at different distances from
 the 
\noun on
Kinect
\noun default
 sensor
\begin_inset CommandInset label
LatexCommand label
name "fig:body-bandwidth-variation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The small scale of the controlled LAN environment does not inherently incur
 long latencies between nodes and as such does not naturally present a valid
 environment for these investigations.
 To test such a scenario, full-scene transmissions were used in order to
 saturate the available bandwidth.
 The connection between nodes also included a WiFi connection which limited
 the available bandwidth to around 100Mbps.
 As the full-scene configuration requires around 275Mbps, artificial latency
 was introduced as frames took longer to deliver and were produced faster
 than they could be transmitted to the server.
 A diagram of this testing layout can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TestLAN"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/TestLAN.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Client-server relationship during initial testing in a LAN environment
\begin_inset CommandInset label
LatexCommand label
name "fig:TestLAN"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The step was initially specified and varied manually in order to visualise
 the response in delay and FPS.
 
\end_layout

\begin_layout Standard
Following the implementation of a dynamic step, the response was again visualise
d using figure-only capture for realistic latency values.
 An exponential moving average of the live frame rate was taken using an
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\alpha$
\end_inset

 of 0.7 in order to smooth any noise.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
The frame drop rate or step, 
\begin_inset Formula $S$
\end_inset

, is defined as the proportion of frames to probabilistically drop at the
 client.
 This was transformed into an expected frame rate (
\begin_inset Formula $\upsilon_{E}$
\end_inset

) using the maximum expected frame rate (
\begin_inset Formula $\upsilon_{max}$
\end_inset

) via the following, 
\begin_inset Formula 
\[
\upsilon_{E}=(1-S)\cdot\upsilon_{max}
\]

\end_inset


\end_layout

\begin_layout Standard
This allows comparison between the live FPS and what the frame drop rate
 should ideally be inducing in this value.
 The 
\noun on
Kinect
\noun default
 sensor captures footage at 30Hz and as such this was used as 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\upsilon_{max}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Natural Environment Evaluations
\end_layout

\begin_layout Standard
In order to properly evaluate the hypothesis in a more natural environment
 for the suite, the server was migrated to a virtual machine running in
 
\noun on
Microsoft
\noun default
's 
\noun on
Azure
\noun default
 cloud computing environment.
 This final testing scenario can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TestCloud"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 A 
\emph on
Standard F2s V2
\emph default
 class virtual machine was used, the specifications for which can be seen
 in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Virtual-Machine-Specifications"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
When operating over the open internet, the domestic internet connection
 became relevant as a limiting factor, specifically the rated upload speed
 of 10Mbps.
 With respect to the desired speeds mentioned above (5 - 275 Mbps), this
 can be seen to be a significant bottleneck.
 As such figure-only capture was used in order to limit the required bandwidth.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/TestCloud.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Client-server relationship during further testing in a cloud-based environment
\begin_inset CommandInset label
LatexCommand label
name "fig:TestCloud"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Subsubsection
LAN Premise Validation
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents a test scenario with a manually defined and varied frame drop
 rate.
 Latency EMA refers to the moving exponential average of latency values
 received from connected clients.
 An 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\alpha$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 of 0.5 was used.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/ManualLatencySmall.png
	lyxscale 40
	width 50col%

\end_inset


\begin_inset Graphics
	filename ../media/graphs/ManualFPSSmall.png
	lyxscale 40
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Latency and FPS response for a manually varied frame drop step (Full-scene
 capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:Latency-and-FPS1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Generally, the lowering of the frame drop proportion beyond a critical value
 induces the latency to begin increasing at a fairly linear rate.
 This can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Latency-and-FPS1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 at times 
\begin_inset Formula $t=133$
\end_inset

 (Increasing at roughly 
\begin_inset Formula $176\,\unitfrac{ms}{s}$
\end_inset

) and 
\begin_inset Formula $t=250$
\end_inset

 (
\begin_inset Formula $\approx151\,\unitfrac{ms}{s}$
\end_inset

).
 Conversely, increasing the step causes a peak in the latency before inducing
 the value to begin dropping.
 There is a noticeable lag in response, changes in latency are offset in
 time from the reception of a changed drop rate.
 Additionally an initial spike in latency can be seen despite having a 50%
 drop rate from 
\begin_inset Formula $t=0$
\end_inset

.
 
\end_layout

\begin_layout Standard
Looking to the FPS chart, the frame rate drops in response to an increase
 in frame drop rate and vice versa.
 Similarly to latency, a lag can be seen when changes in drop rate are received.
 Worth noticing is that as the step drops, the FPS increases before the
 latency begins increasing (
\begin_inset Formula $t=70$
\end_inset

 to 
\begin_inset Formula $t=133$
\end_inset

).
 From here, the frame drop proportion or step is presented through the previousl
y defined expected frame rate.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Local-Dynamic"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the variation in latency following the implementation of a dynamic
 step, an exponential 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\alpha$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 of 0.7 was used for both EMAs.
 Both charts show segments of a single test case, the entire dataset can
 be seen visualised in appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Dynamic-Step-Test"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It can be seen that as the average latency (
\color blue
blue
\color inherit
) increases above the prescribed latency requirement (
\color orange
yellow
\color inherit
) the dynamic step induces a drop in expected frame rate (
\color green
green
\color inherit
).
 As the latency falls past the requirement, the expected frame rate rises
 back to the maximum, each latency spike has an associated frame rate drop.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/LocalDynamic1000FPSEMA0.7Labelled.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
1000ms latency requirement, FPS EMA 
\begin_inset Formula $\alpha$
\end_inset

 = Latency EMA 
\begin_inset Formula $\alpha$
\end_inset

 = 0.7
\begin_inset CommandInset label
LatexCommand label
name "fig:1000ms-latency-requirement"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/LocalDynamic2000FPSEMA0.7Labelled.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
2000ms latency requirement, FPS EMA 
\begin_inset Formula $\alpha$
\end_inset

 = Latency EMA 
\begin_inset Formula $\alpha$
\end_inset

 = 0.7
\begin_inset CommandInset label
LatexCommand label
name "fig:2000ms-latency-requirement"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Segments of data demonstrating dynamic frame rate response to different
 latency requirements in a LAN environment, note the difference in latency
 scale during comparison (Figure-only capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:Local-Dynamic"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When comparing the expected frame rate with what was actually received (
\color red
red
\color inherit
), the live data can, in general, be seen to follow the intended value as
 expected.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2000ms-latency-requirement"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows higher latency peaks (
\begin_inset Formula $\approx$
\end_inset

6000-10,000ms compared to 
\begin_inset Formula $\approx$
\end_inset

2,000-3,000ms) for a higher requirement, the expected frame rate can also
 be seen to drop lower (
\begin_inset Formula $\approx$
\end_inset

1.5 FPS compared to 
\begin_inset Formula $\approx$
\end_inset

7.5 FPS).
\end_layout

\begin_layout Standard
Many of the induced frame rate drops display a lag in responding to a latency
 increase, six latency spikes can be seen presented individually in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lag-spike-details"
plural "false"
caps "false"
noprefix "false"

\end_inset

, letter identifiers relate to those seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Local-Dynamic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 While the extent of the lag varies, in the figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:a-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:c-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 there can be seen to be roughly 10 seconds between the expected rate intersecti
ng the actual FPS and the induced drop.
 Additionally, in these cases the expected frame rate has already started
 increasing before the actual frame rate properly responds.
\end_layout

\begin_layout Standard
Worth noting is the difference in lag when the expected frame rate increases
 as opposed to when it is decreasing.
 Visible in much of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Local-Dynamic"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lag-spike-details"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is that the lag in frame rate decrease is generally not visible when the
 frame rate increases, the actual value more closely follows the expected
 value.
\end_layout

\begin_layout Standard
As a whole, in general the observed frame rate can be seen initially to
 drop with the expected value before flattening and peaking at a similar
 time to latency.
 Latency and frame rate in many cases drop at similar times before the FPS
 then rises in line with the expected value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:a-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall3.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:b-3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall2.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:c-2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall5.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:d-5"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall4.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:e-4"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/FPSLagSmall6.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:f-6"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Details of individual latency spikes highlighting the lag between live and
 expected FPS values
\begin_inset CommandInset label
LatexCommand label
name "fig:lag-spike-details"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Cloud Environment
\begin_inset CommandInset label
LatexCommand label
name "subsec:Cloud-Results"

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:raw-cloud-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the average transmission latency with an uncontrolled frame drop
 rate.
 The used bandwidth is also visualised averaging around 10Mbps, the rated
 upload speed for the domestic internet connection.
 The latency can be seen to increase linearly and constantly with the exponentia
l and simple moving averages increasing at 
\begin_inset Formula $500\,\unitfrac{ms}{s}$
\end_inset

 and 
\begin_inset Formula $460\,\unitfrac{ms}{s}$
\end_inset

 respectively.
 The exponential moving average can be seen to move faster in response to
 new values, a feature also seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:unstable-cloud-based-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as the early latency spike rises higher and drops off faster than the SMA
 which has a smoother response.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/RawLatencyWithoutStep.png
	lyxscale 40
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Average transmission latency with no client frame drop (Figure-only capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:raw-cloud-latency"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:unstable-cloud-based-latency"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is presented to highlight the instability encountered in the cloud-based
 environment.
 Similarly to the local environment, an initial spike in latency is seen
 before reducing to a consistent value.
 When the frame drop rate is dropped from 0.8 to 0.6, increasing the expected
 frame rate, the connection becomes immediately unstable with bandwidth
 dropping to 0 Mbps as the connection is lost.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/CloudInstabilityLatency.png
	lyxscale 40
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/CloudInstabilityBandwidth.png
	lyxscale 40
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cloud-based latency and bandwidth as a function of a manual frame drop step
 (Figure-only capture)
\begin_inset CommandInset label
LatexCommand label
name "fig:unstable-cloud-based-latency"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Standard
Demonstrated in both the LAN and cloud environment, it can be seen that
 a high enough frame drop rate maintains a constant latency while lowering
 this beyond a critical value induces a constant linear increase.
 This would suggest that below this critical value the server is not able
 to receive frames at a rate higher than they are being produced at the
 client allowing the latency to begin increasing as a backlog of frames
 is produced.
\end_layout

\begin_layout Subsubsection
LAN Premise Validation
\end_layout

\begin_layout Standard
Key in understanding figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Local-Dynamic"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lag-spike-details"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is noting that the latency values measure the time between a frame being
 queued at the client and it being received at the server.
 The previously mentioned inability to retrieve frames faster than they
 are being produced allows the transmission buffer at the client to begin
 filling.
 While it may at first appear odd that the observed frame rate peaks as
 latency peaks, this can be explained by attributing this to the time where
 the network conditions are alleviating and the transmission buffer is emptying
 in a flushing motion.
 Hence, the point at which the latency peaks is not the point at which the
 network is being stressed most, this would be just beforehand when the
 latency is increasing.
 With an empty buffer, the latency drops and the frame rate falls to the
 expected value, more able to closely follow it with the less contested
 network conditions.
\end_layout

\begin_layout Standard
Following the implementation of a dynamic step, the latency is no longer
 unbounded and instead oscillates as the frame rate increases and decreases.
 As a result, the system can be seen to dynamically drop frames in order
 to maintain a constant latency in adverse network conditions.
 This allows the system to keep up when the amount of frames being produced
 by the client is too high to be transmitted in the same period of time.
\end_layout

\begin_layout Standard
The performance of this system, however, should be considered.
 With a 1000ms requirement, the latency can be seen to rise to 2000ms and
 as high as 3000ms before the step begins mitigating this rise.
 This difference is worse for the 2000ms requirement (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2000ms-latency-requirement"
plural "false"
caps "false"
noprefix "false"

\end_inset

) with the latency reaching as high as 10,000ms, a 10 second delay between
 capture and display could pose significant issues if the application is
 intended to be real-time or interactive.
 The level to which the latency swings above the requirement is significant
 and is likely in part due to the linear rate with which the step increases
 and decreases.
\end_layout

\begin_layout Standard
One method to control this would be to have the step move in a non-linear
 fashion, changing at a rate proportional to the difference between the
 live value and the requirement.
 This could allow the latency to oscillate closer to the requirement with
 the goal being for it to settle at this value.
\end_layout

\begin_layout Standard
Another contributor to this spike could be a delay in delivering new drop
 rates observed during testing.
 Although the rate moved in real-time at the server, sometimes this did
 not appear to be the case at the client.
 As particular strain was put on the connection, specifically as the latency
 increased, the ability to deliver new drop rates appeared to be compromised,
 this would explain why the lag was seen more as the latency increased and
 less so when the latency is low (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lag-spike-details"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 As this is included in the header of each request it was concluded that
 requests were being queued at the server unable to be transmitted over
 the socket busy receiving frames.
\end_layout

\begin_layout Subsubsection
Cloud Environment
\end_layout

\begin_layout Standard
With the employed method of controlling transmission latency, representative
 data was not able to be gathered in the cloud environment due to connection
 instability.
 While this was partly due to the suite's sensitivity to poor network conditions
 as a result of its initial design favouring a LAN environment, the implemented
 latency control method was likely also a factor.
\end_layout

\begin_layout Standard
The need to deliver a step from the server to each client created an initial
 spike in latency in both environments but was more pronounced in the cloud.
 The previously mentioned issues with promptly delivering new frame drop
 rates was more pronounced in this scenario.
\end_layout

\begin_layout Standard
This highlights one of the downsides of the implemented control method.
 Requiring a parameter to be delivered to the client for use requires reliable
 delivery of this parameter.
 This is not ideal when the parameter used to mitigate adverse network condition
s may itself be susceptible to such network conditions.
\end_layout

\begin_layout Standard
An alternative would be to allow the server to independently implement this
 frame rate control.
 This could be done by dynamically controlling the rate of frame requests
 made by the server as opposed to the rate of frames created at the client.
 An advantage could be found in the increased efficiency.
 Currently the server requests frames at a constant rate and the client
 decides whether a frame should be delivered in response.
 With a higher step (and therefore lower expected frame rate) a higher proportio
n of requests made by the server are unnecessary.
\end_layout

\begin_layout Subsection
Summary
\end_layout

\begin_layout Standard
In this section, investigations were made into controlling the otherwise
 unbounded display latency in a LAN and cloud-based environment.
 This was successfully demonstrated in the LAN environment and the performance
 was discussed, the linear movement of the frame drop rate was shown to
 induce a slow response that allow high peaks in latency.
 
\end_layout

\begin_layout Standard
The cloud environment was shown to be highly unstable, such that representative
 data was not able to be collected.
 This was suggested to be partly due to the nature of the suite's behaviour
 and partly due to unreliability in transmitting the frame drop rate.
 Alternatives to the implemented client frame drop rate that could alleviate
 this were presented, allowing the server to independently implement the
 control method.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Future Work
\begin_inset CommandInset label
LatexCommand label
name "sec:Future-Work"

\end_inset


\end_layout

\begin_layout Standard
Below outlines possible future developments to the 
\noun on
LiveScan3D
\noun default
 suite with regards to both the work presented in this project and the suite
 as a whole.
\end_layout

\begin_layout Standard
As presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Source-Synchronisation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, implementing forms of 
\emph on
active
\emph default
 synchronisation were hindered by the lack of access to a live multi-source
 environment.
 With access to multiple 
\noun on
Kinect
\noun default
 sensors, a method to tightly match different source frames within the processin
g pipeline based on timestamp could be implemented and tested.
\end_layout

\begin_layout Standard
Following initial investigations into the effect of controlling frame rate
 on effective latency, the implemented method of delivering a frame drop
 rate was found to be susceptible to poor network conditions.
 Future investigations could be made into alternative methods for achieving
 this described balance, one method for doing so could be allowing the server
 to dynamically manage its request rate of clients.
 This would require allowing the client transmission buffer to have a rolling
 behaviour where new frames result in a dropping of the oldest.
\end_layout

\begin_layout Standard
The migration of the mobile application's AR environment in 
\noun on
Google
\noun default
's 
\noun on
ARCore
\noun default
 to that of 
\noun on
Unity
\noun default
's native 
\noun on
ARFoundation
\noun default
 framework allowed deploying to both iOS and Android phones.
 However, the previously used geometry shader is not compatible with the
 iOS 
\noun on
Metal
\noun default
 graphics library, limiting functionality to just that of the network layer.
 With a compatible shader, the cross-platform migration would be feature-complet
e, extending the possible install base of the application while also providing
 access to the currently wider feature set of iOS's 
\noun on
ARKit
\noun default
.
\end_layout

\begin_layout Standard
With investigations being made into the speed and latency of network communicati
ons, this is an area with much space for development.
 The use of the TCP protocol for frame delivery includes overhead compared
 to the more standard UDP for interactive media applications.
 Migrating to UDP could present opportunities to increase the network speed.
\end_layout

\begin_layout Standard
There are many ways in which the packet size could be reduced, one already
 supported by the suite is the use of 
\noun on
ZStandard
\begin_inset CommandInset citation
LatexCommand cite
key "zstd"
literal "false"

\end_inset


\noun default
 compression, a lossless compression algorithm developed at 
\noun on
Facebook
\noun default
.
 A similar method could be the use of hologram sub-sampling, particularly
 between the server and user experience.
 This would involve reducing the size of a transmitted holograms by sampling
 only a percentage of the point cloud vertices and increasing the point
 size at display to cover the space in-between.
 When presented at a small scale at the user experience this could present
 a method to reduce transmission bandwidth while maintaining much of the
 final experience.
\end_layout

\begin_layout Standard
Another method for achieving this reduction in bandwidth could be inherited
 from traditional video compression through the use of 
\emph on
frame types
\emph default
 including I-frames, P-frames and B-frames.
 Essentially, when capturing a whole scene it is unlikely that all of it
 will change significantly each frame and as such calculating and transmitting
 only the changes since the last could significantly reduce the required
 bandwidth.
 The challenge here would be establishing temporal coherence between successive
 frames to identify these differences, a harder task in 3D.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Summary
\begin_inset CommandInset label
LatexCommand label
name "sec:Summary"

\end_inset


\end_layout

\begin_layout Standard
Within this piece, the process of extending the 
\noun on
LiveScan3D
\noun default
 software to include multi-source holoportation has been presented.
 The use of such a system has many applications from those inherited from
 traditional 2D video such as conference calls to wholly unique applications
 within the new domain.
\end_layout

\begin_layout Standard
The literature review contextualises the 
\noun on
LiveScan
\noun default
 suite within the wider spaces of XR, volumetric video and multi-source
 holoportation itself.
 Previous examples of holoportation are presented and their aims of achieving
 telepresence are discussed.
\end_layout

\begin_layout Standard
The 
\noun on
LiveScan3D
\noun default
 suite is described, both in its initial state and following developments
 made to the network behaviour to include a sub-system of buffers.
 Both accompanying AR applications for the suite are presented and finally
 the suite as a whole is evaluated with its strengths and limitations highlighte
d.
\end_layout

\begin_layout Standard
The multi-source developments made throughout the suite are presented, the
 server's native 
\noun on
OpenGL
\noun default
 viewfinder was updated in order to allow the concurrent display of multiple
 sources through a sub-system of dynamic geometric transformations.
\end_layout

\begin_layout Standard
A source ID was introduced in order to identify hologram frames during transmiss
ion over the network.
 The updated packet structure was presented with the location of the aforementio
ned source ID highlighted.
\end_layout

\begin_layout Standard
Active and passive source synchronisation was introduced in an effort to
 allow the coarse and fine temporal matching of different source's frames.
\end_layout

\begin_layout Standard
The updated structure of the mobile AR application was described with the
 ability to display multiple sources presented and the limitations highlighted.
\end_layout

\begin_layout Standard
Additional behaviour designed to facilitate the server's position as a manager
 of multiple source sessions was introduced including 
\emph on
stale sources
\emph default
 and the management of settings per-source.
\end_layout

\begin_layout Standard
Efforts made to migrate the mobile application to a cross-platform AR environmen
t were outlined.
 The progress made and its implications on the remaining project were described
 while the remaining graphics programming related objectives are presented.
\end_layout

\begin_layout Standard
A system of frame rate throttling was successfully implemented in order
 to investigate the effects on effective display latency.
 It's applicability to source synchronisation was discussed and preliminary
 data from both a local and cloud-based environment was presented.
 The implemented method was successfully shown to limit latency by controlling
 transmitted frame rate.
 Possible drawbacks with the method were highlighted and alternative implementat
ions were proposed.
\end_layout

\begin_layout Standard
Possible future developments and investigations were outlined including
 those working from the developments of this project and those inherent
 to the suite itself.
\end_layout

\begin_layout Section
Conclusions
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusions"

\end_inset


\end_layout

\begin_layout Standard
Holoportation and multi-source configurations thereof are important technologies
 for extended reality experiences with broad appeal to many applications.
 The use of consumer hardware, specifically the 
\noun on
Kinect
\noun default
, has accelerated the space.
\end_layout

\begin_layout Standard

\noun on
LiveScan3D
\noun default
 presents an open-source, domain-agnostic holoportation solution and a multi-sou
rce version can be seen to further generalise the application suite.
\end_layout

\begin_layout Standard
Unfortunately, the impact of the COVID-19 pandemic severely impacted the
 latter third of the project resulting in an inability to both quantitatively
 evaluate the multi-source server and implement the mobile AR application
 touch controls.
\end_layout

\begin_layout Standard
Despite the obstacles, additional objectives allowed extra investigations
 to be made into the suite with significant progress made in making the
 mobile application cross-platform and data gathered on the network behaviour.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bibliography"

\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "/home/andy/uni/dissertation/references"
options "bibtotoc"

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Project Gantt Chart
\begin_inset CommandInset label
LatexCommand label
name "sec:Gantt-Chart"

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../media/GanttChart.png
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Section
Server UI Additions
\begin_inset CommandInset label
LatexCommand label
name "sec:Server-UI-Additions"

\end_inset


\end_layout

\begin_layout Standard
During the development of the network behaviour of the suite, various UI
 elements were added to the server application in order to aid in debugging.
 The final UI can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LiveScan-server-UI"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/ServerWindow.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
LiveScan server window following the addition of UI elements for viewing
 buffer capacity, network bandwidth and operating frequencies
\begin_inset CommandInset label
LatexCommand label
name "fig:LiveScan-server-UI"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Working clockwise around the group boxes the new elements are as follows,
\end_layout

\begin_layout Itemize
Receiving and transmitting network bandwidth 
\end_layout

\begin_layout Itemize
Buffer population
\end_layout

\begin_layout Itemize
Operating frequencies
\end_layout

\begin_layout Itemize
Connected user experiences
\end_layout

\begin_layout Itemize
Connected sources with IDs
\end_layout

\begin_layout Itemize
Dynamic step options including exponential moving average alpha and size
 of latency queue
\end_layout

\begin_layout Section
Network Message Types
\begin_inset CommandInset label
LatexCommand label
name "sec:Network-Message-Types"

\end_inset


\end_layout

\begin_layout Itemize
MSG_CAPTURE_FRAME
\end_layout

\begin_layout Itemize
MSG_CALIBRATE
\end_layout

\begin_layout Itemize
MSG_RECEIVE_SETTINGS
\end_layout

\begin_layout Itemize
MSG_REQUEST_STORED_FRAME
\end_layout

\begin_layout Itemize
MSG_REQUEST_LAST_FRAME
\end_layout

\begin_layout Itemize
MSG_RECEIVE_CALIBRATION
\end_layout

\begin_layout Itemize
MSG_CLEAR_STORED_FRAMES
\end_layout

\begin_layout Itemize
MSG_CONFIRM_CAPTURED
\end_layout

\begin_layout Itemize
MSG_CONFIRM_CALIBRATED
\end_layout

\begin_layout Itemize
MSG_SEND_STORED_FRAME
\end_layout

\begin_layout Itemize
MSG_SEND_LAST_FRAME
\end_layout

\begin_layout Itemize
MSG_NO_FRAME 
\end_layout

\begin_layout Section
Frame Data Structure
\begin_inset CommandInset label
LatexCommand label
name "sec:Frame"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand lstinputlisting
filename "../snippets/frame.cs"
lstparams "language={[Sharp]C},caption={Point cloud with Client ID}"

\end_inset


\end_layout

\begin_layout Section
Linear Frame Drop Rate Calculation
\begin_inset CommandInset label
LatexCommand label
name "sec:Linear-Frame-Drop"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

if (fpsRequirement > 0) // fps constrained step
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

	if (source.Value.FPS > fpsRequirement)
\end_layout

\begin_layout Plain Layout

		source.Value.Step -= StepIncrement;
\end_layout

\begin_layout Plain Layout

	else
\end_layout

\begin_layout Plain Layout

		source.Value.Step += StepIncrement;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

else if (latencyRequirement > 0) // latency constrained step
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

	if (source.Value.LatencyEMA > latencyRequirement)
\end_layout

\begin_layout Plain Layout

		source.Value.Step += StepIncrement;
\end_layout

\begin_layout Plain Layout

	else
\end_layout

\begin_layout Plain Layout

		source.Value.Step -= StepIncrement;
\end_layout

\begin_layout Plain Layout

}                          
\end_layout

\begin_layout Plain Layout

else                             
\end_layout

\begin_layout Plain Layout

	source.Value.Step = 0;
\end_layout

\end_inset


\end_layout

\begin_layout Section
Dynamic Step Test Results
\begin_inset CommandInset label
LatexCommand label
name "sec:Dynamic-Step-Test"

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../media/graphs/LocalDynamicTotalFPSEMA0.7.png
	lyxscale 30
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Note how without a latency requirement (
\begin_inset Formula $t=290$
\end_inset

) the latency increases in an uncontrolled fashion.
\end_layout

\begin_layout Section
Virtual Machine Specifications
\begin_inset CommandInset label
LatexCommand label
name "sec:Virtual-Machine-Specifications"

\end_inset


\end_layout

\begin_layout Standard

\emph on
Standard F2s v2
\emph default
 specifications found at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout
docs.microsoft.com/en-us/azure/virtual-machines/fsv2-series
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Based on the Intel Xeon Platinum 8168.
 It features a sustained all core Turbo clock speed of 3.4 GHz and a maximum
 single-core turbo frequency of 3.7 GHz.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vCPUs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Memory 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 GiB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Temp Storage (SSD)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16 GiB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Max Data Disks
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Expected Network Bandwidth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
875 Mbps
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_body
\end_document
